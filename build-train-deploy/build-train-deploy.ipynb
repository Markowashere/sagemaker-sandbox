{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32032a5-c5e5-4ecc-8992-5c16dfd08c69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version is good\n",
      "Region = us-east-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import sys\n",
    "import IPython\n",
    "\n",
    "if int(sagemaker.__version__.split('.')[0]) == 2:\n",
    "    print(\"Installing previous SageMaker Version and restarting the kernel\")\n",
    "    !{sys.executable} -m pip install sagemaker==1.72.0\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "else:\n",
    "    print(\"Version is good\")\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.session.Session().region_name\n",
    "print(\"Region = {}\".format(region))\n",
    "sm = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eabfda4-4c68-4e57-b007-1ba05531095e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.7/site-packages (0.1.43)\n",
      "Requirement already satisfied: boto3>=1.16.27 in /opt/conda/lib/python3.7/site-packages (from sagemaker-experiments) (1.26.111)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.111 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.29.111)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.111->boto3>=1.16.27->sagemaker-experiments) (1.26.15)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.111->boto3>=1.16.27->sagemaker-experiments) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.111->boto3>=1.16.27->sagemaker-experiments) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import sleep, gmtime, strftime\n",
    "import json\n",
    "import time\n",
    "\n",
    "#experiments\n",
    "!pip install sagemaker-experiments \n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2091ba2-0bd0-4405-8680-3f251f5ae255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rawbucket= sess.default_bucket() # Alternatively you can use our custom bucket here. \n",
    "\n",
    "prefix = 'sagemaker-modelmonitor' # use this prefix to store all files pertaining to this workshop.\n",
    "\n",
    "dataprefix = prefix + '/data'\n",
    "traindataprefix = prefix + '/train_data'\n",
    "testdataprefix = prefix + '/test_data'\n",
    "testdatanolabelprefix = prefix + '/test_data_no_label'\n",
    "trainheaderprefix = prefix + '/train_headers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604a8b8b-19f8-4bcd-a6c0-cd320e5f9945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-17 06:14:56--  https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5539328 (5.3M) [application/x-httpd-php]\n",
      "Saving to: ‘default of credit card clients.xls.3’\n",
      "\n",
      "default of credit c 100%[===================>]   5.28M  9.43MB/s    in 0.6s    \n",
      "\n",
      "2023-04-17 06:14:57 (9.43 MB/s) - ‘default of credit card clients.xls.3’ saved [5539328/5539328]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0      20000    2          2         1   24      2      2     -1     -1   \n",
       "1     120000    2          2         2   26     -1      2      0      0   \n",
       "2      90000    2          2         2   34      0      0      0      0   \n",
       "3      50000    2          2         1   37      0      0      0      0   \n",
       "4      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0     -2  ...          0          0          0         0       689         0   \n",
       "1      0  ...       3272       3455       3261         0      1000      1000   \n",
       "2      0  ...      14331      14948      15549      1518      1500      1000   \n",
       "3      0  ...      28314      28959      29547      2000      2019      1200   \n",
       "4      0  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "0         0         0         0                           1  \n",
       "1      1000         0      2000                           1  \n",
       "2      1000      1000      5000                           0  \n",
       "3      1100      1069      1000                           0  \n",
       "4      9000       689       679                           0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\n",
    "data = pd.read_excel('default of credit card clients.xls', header=1)\n",
    "data = data.drop(columns = ['ID'])\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e6b67a-2a0b-4825-a4c9-597f5b40590e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2682</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13559</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49291</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>35835</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  \\\n",
       "0      1      20000    2          2         1   24      2      2     -1   \n",
       "1      1     120000    2          2         2   26     -1      2      0   \n",
       "2      0      90000    2          2         2   34      0      0      0   \n",
       "3      0      50000    2          2         1   37      0      0      0   \n",
       "4      0      50000    1          2         1   57     -1      0     -1   \n",
       "\n",
       "   PAY_4  ...  BILL_AMT3  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
       "0     -1  ...        689          0          0          0         0       689   \n",
       "1      0  ...       2682       3272       3455       3261         0      1000   \n",
       "2      0  ...      13559      14331      14948      15549      1518      1500   \n",
       "3      0  ...      49291      28314      28959      29547      2000      2019   \n",
       "4      0  ...      35835      20940      19146      19131      2000     36681   \n",
       "\n",
       "   PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \n",
       "0         0         0         0         0  \n",
       "1      1000      1000         0      2000  \n",
       "2      1000      1000      1000      5000  \n",
       "3      1200      1100      1069      1000  \n",
       "4     10000      9000       689       679  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rename(columns={\"default payment next month\": \"Label\"}, inplace=True)\n",
    "lbl = data.Label\n",
    "data = pd.concat([lbl, data.drop(columns=['Label'])], axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab8e7d34-d4a5-4155-bc59-c84c5c106523",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/data\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('rawdata/rawdata.csv'):\n",
    "    !mkdir rawdata\n",
    "    data.to_csv('rawdata/rawdata.csv', index=None)\n",
    "else:\n",
    "    pass\n",
    "# Upload the raw dataset\n",
    "raw_data_location = sess.upload_data('rawdata', bucket=rawbucket, key_prefix=dataprefix)\n",
    "print(raw_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3f862-3c78-4490-8974-0e02a539a6a1",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8444f0b0-e95b-4542-a0f1-7ec2d8652814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.c4.xlarge',\n",
    "                                     instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59bb791e-1ba6-4a1b-a5b4-80d955ae50ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-test-split-ratio', type=float, default=0.3)\n",
    "    parser.add_argument('--random-split', type=int, default=0)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print('Received arguments {}'.format(args))\n",
    "\n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', 'rawdata.csv')\n",
    "    \n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    df.sample(frac=1)\n",
    "    \n",
    "    COLS = df.columns\n",
    "    newcolorder = ['PAY_AMT1','BILL_AMT1'] + list(COLS[1:])[:11] + list(COLS[1:])[12:17] + list(COLS[1:])[18:]\n",
    "    \n",
    "    split_ratio = args.train_test_split_ratio\n",
    "    random_state=args.random_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('Label', axis=1), df['Label'], \n",
    "                                                        test_size=split_ratio, random_state=random_state)\n",
    "    \n",
    "    preprocess = make_column_transformer(\n",
    "        (['PAY_AMT1'], StandardScaler()),\n",
    "        (['BILL_AMT1'], MinMaxScaler()),\n",
    "    remainder='passthrough')\n",
    "    \n",
    "    print('Running preprocessing and feature engineering transformations')\n",
    "    train_features = pd.DataFrame(preprocess.fit_transform(X_train), columns = newcolorder)\n",
    "    test_features = pd.DataFrame(preprocess.transform(X_test), columns = newcolorder)\n",
    "    \n",
    "    # concat to ensure Label column is the first column in dataframe\n",
    "    train_full = pd.concat([pd.DataFrame(y_train.values, columns=['Label']), train_features], axis=1)\n",
    "    test_full = pd.concat([pd.DataFrame(y_test.values, columns=['Label']), test_features], axis=1)\n",
    "    \n",
    "    print('Train data shape after preprocessing: {}'.format(train_features.shape))\n",
    "    print('Test data shape after preprocessing: {}'.format(test_features.shape))\n",
    "    \n",
    "    train_features_headers_output_path = os.path.join('/opt/ml/processing/train_headers', 'train_data_with_headers.csv')\n",
    "    \n",
    "    train_features_output_path = os.path.join('/opt/ml/processing/train', 'train_data.csv')\n",
    "    \n",
    "    test_features_output_path = os.path.join('/opt/ml/processing/test', 'test_data.csv')\n",
    "    \n",
    "    print('Saving training features to {}'.format(train_features_output_path))\n",
    "    train_full.to_csv(train_features_output_path, header=False, index=False)\n",
    "    print(\"Complete\")\n",
    "    \n",
    "    print(\"Save training data with headers to {}\".format(train_features_headers_output_path))\n",
    "    train_full.to_csv(train_features_headers_output_path, index=False)\n",
    "                 \n",
    "    print('Saving test features to {}'.format(test_features_output_path))\n",
    "    test_full.to_csv(test_features_output_path, header=False, index=False)\n",
    "    print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f498f3e-9ccf-4df6-a740-d740f972cf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/code/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "# Copy the preprocessing code over to the s3 bucket\n",
    "codeprefix = prefix + '/code'\n",
    "codeupload = sess.upload_data('preprocessing.py', bucket=rawbucket, key_prefix=codeprefix)\n",
    "print(codeupload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0448f598-16a6-45c7-9609-ffabf5dd3f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data location = sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/train_data\n",
      "Test data location = sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/test_data\n"
     ]
    }
   ],
   "source": [
    "train_data_location = rawbucket + '/' + traindataprefix\n",
    "test_data_location = rawbucket+'/'+testdataprefix\n",
    "print(\"Training data location = {}\".format(train_data_location))\n",
    "print(\"Test data location = {}\".format(test_data_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7fae014-adc2-4b2e-b1ab-4400b58137e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sagemaker-scikit-learn-2023-04-17-06-15-00-482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2023-04-17-06-15-00-482\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/data', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/train_data', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/test_data', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'train_data_headers', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/train_headers', 'LocalPath': '/opt/ml/processing/train_headers', 'S3UploadMode': 'EndOfJob'}}]\n",
      "..............................\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\u001b[0m\n",
      "\u001b[34mReceived arguments Namespace(random_split=0, train_test_split_ratio=0.2)\u001b[0m\n",
      "\u001b[34mReading input data from /opt/ml/processing/input/rawdata.csv\u001b[0m\n",
      "\u001b[34mRunning preprocessing and feature engineering transformations\u001b[0m\n",
      "\u001b[34mTrain data shape after preprocessing: (24000, 23)\u001b[0m\n",
      "\u001b[34mTest data shape after preprocessing: (6000, 23)\u001b[0m\n",
      "\u001b[34mSaving training features to /opt/ml/processing/train/train_data.csv\u001b[0m\n",
      "\u001b[34mComplete\u001b[0m\n",
      "\u001b[34mSave training data with headers to /opt/ml/processing/train_headers/train_data_with_headers.csv\u001b[0m\n",
      "\u001b[34mSaving test features to /opt/ml/processing/test/test_data.csv\u001b[0m\n",
      "\u001b[34mComplete\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(code=codeupload,\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=raw_data_location,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/train',\n",
    "                               destination='s3://' + train_data_location),\n",
    "                               ProcessingOutput(output_name='test_data',\n",
    "                                                source='/opt/ml/processing/test',\n",
    "                                               destination=\"s3://\"+test_data_location),\n",
    "                               ProcessingOutput(output_name='train_data_headers',\n",
    "                                                source='/opt/ml/processing/train_headers',\n",
    "                                               destination=\"s3://\" + rawbucket + '/' + prefix + '/train_headers')],\n",
    "                      arguments=['--train-test-split-ratio', '0.2']\n",
    "                     )\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train_data':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'test_data':\n",
    "        preprocessed_test_data = output['S3Output']['S3Uri']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31d8f9-f522-4e73-a038-6b99a9235f9e",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "229ca7a7-bbde-46c6-8048-1103e724b095",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7ff89b726190>,experiment_name='Build-train-deploy-1681712445',description='Predict credit card default from payments data',tags=None,experiment_arn='arn:aws:sagemaker:us-east-1:477728513638:experiment/build-train-deploy-1681712445',response_metadata={'RequestId': '8d61a65e-6962-493b-a541-484cc320df78', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '8d61a65e-6962-493b-a541-484cc320df78', 'content-type': 'application/x-amz-json-1.1', 'content-length': '101', 'date': 'Mon, 17 Apr 2023 06:20:45 GMT'}, 'RetryAttempts': 0})\n"
     ]
    }
   ],
   "source": [
    "# Create a SageMaker Experiment\n",
    "cc_experiment = Experiment.create(\n",
    "    experiment_name=f\"Build-train-deploy-{int(time.time())}\", \n",
    "    description=\"Predict credit card default from payments data\", \n",
    "    sagemaker_boto_client=sm)\n",
    "print(cc_experiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc4c3fd-85e4-40ef-9da1-259c88492542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start Tracking parameters used in the Pre-processing pipeline.\n",
    "with Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n",
    "    tracker.log_parameters({\n",
    "        \"train_test_split_ratio\": 0.2,\n",
    "        \"random_state\":0\n",
    "    })\n",
    "    # we can log the s3 uri to the dataset we just uploaded\n",
    "    tracker.log_input(name=\"ccdefault-raw-dataset\", media_type=\"s3/uri\", value=raw_data_location)\n",
    "    tracker.log_input(name=\"ccdefault-train-dataset\", media_type=\"s3/uri\", value=train_data_location)\n",
    "    tracker.log_input(name=\"ccdefault-test-dataset\", media_type=\"s3/uri\", value=test_data_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "757eebbc-cc85-4af1-a4b4-0c00420efcd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.amazon.amazon_estimator:'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker.estimator:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating training-job with name: cc-training-job-1681715967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-17 07:19:27 Starting - Starting the training job...\n",
      "2023-04-17 07:19:55 Starting - Preparing the instances for training.........\n",
      "2023-04-17 07:21:10 Downloading - Downloading input data...\n",
      "2023-04-17 07:21:37 Training - Downloading the training image...\n",
      "2023-04-17 07:22:23 Training - Training image download completed. Training in progress...\n",
      "2023-04-17 07:22:59 Uploading - Uploading generated training model\n",
      "2023-04-17 07:22:59 Completed - Training job completed\n",
      "\u001b[34m[2023-04-17 07:22:40.039 ip-10-0-104-150.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 24000 rows\u001b[0m\n",
      "\u001b[34m[07:22:40] 24000x23 matrix with 552000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.17854\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.17746\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.17704\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.17717\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.17621\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.17608\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.17504\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.17467\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.17513\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.17475\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.17408\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.17354\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.17363\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.17313\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.17325\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.17296\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.17292\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.17317\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.17250\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.17200\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.17175\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.17142\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.17154\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.17171\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.17112\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.17033\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.17046\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.17029\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.17029\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.17029\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.17021\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.16996\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.16971\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.16929\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.16917\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.16908\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.16904\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.16900\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.16867\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.16846\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.16825\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.16817\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.16817\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.16800\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.16804\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.16804\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.16792\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.16737\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.16717\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.16654\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.16667\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.16642\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.16629\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.16542\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.16517\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.16529\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.16537\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.16537\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.16537\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.16529\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.16521\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.16500\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.16500\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.16487\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.16408\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.16400\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.16387\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.16387\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.16387\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.16375\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.16367\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.16333\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.16337\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.16317\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.16296\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.16287\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.16308\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.16300\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.16300\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.16271\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.16246\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.16237\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.16221\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.16171\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.16179\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.16096\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.16079\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.16067\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.16071\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.16042\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.16017\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.16000\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.16029\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.16012\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.16025\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.15975\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.15983\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.15967\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.15937\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.15887\u001b[0m\n",
      "Training seconds: 112\n",
      "Billable seconds: 112\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', '1.0-1')\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://' + train_data_location, content_type='csv')\n",
    "preprocessing_trial_component = tracker.trial_component\n",
    "\n",
    "trial_name = f\"cc-default-training-job-{int(time.time())}\"\n",
    "cc_trial = Trial.create(\n",
    "        trial_name=trial_name, \n",
    "            experiment_name=cc_experiment.experiment_name,\n",
    "        sagemaker_boto_client=sm\n",
    "    )\n",
    "\n",
    "cc_trial.add_trial_component(preprocessing_trial_component)\n",
    "cc_training_job_name = \"cc-training-job-{}\".format(int(time.time()))\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    train_max_run=86400,\n",
    "                                    output_path='s3://{}/{}/models'.format(rawbucket, prefix),\n",
    "                                    sagemaker_session=sess) # set to true for distributed training\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        verbosity=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit(inputs = {'train':s3_input_train},\n",
    "       job_name=cc_training_job_name,\n",
    "        experiment_config={\n",
    "            \"TrialName\": cc_trial.trial_name, #log training job in Trials for lineage\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        },\n",
    "        wait=True,\n",
    "    )\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f7caf-6141-41be-ae11-50c9a5dae47c",
   "metadata": {},
   "source": [
    "# Deploy for offline inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6457c50-c44b-4328-ba7d-8a4d52d83355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/test_data/test_data.csv to ./test_data.csv\n"
     ]
    }
   ],
   "source": [
    "test_data_path = 's3://' + test_data_location + '/test_data.csv'\n",
    "! aws s3 cp $test_data_path .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "834b298b-7fe4-4d70-8c04-150a52f0466b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.341476</td>\n",
       "      <td>0.201175</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17399.0</td>\n",
       "      <td>19057.0</td>\n",
       "      <td>18453.0</td>\n",
       "      <td>19755.0</td>\n",
       "      <td>19288.0</td>\n",
       "      <td>2260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.136859</td>\n",
       "      <td>0.199594</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19347.0</td>\n",
       "      <td>18600.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.284364</td>\n",
       "      <td>0.185736</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2864.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2873.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.040569</td>\n",
       "      <td>0.289360</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>99998.0</td>\n",
       "      <td>16138.0</td>\n",
       "      <td>17758.0</td>\n",
       "      <td>18774.0</td>\n",
       "      <td>20272.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.079132</td>\n",
       "      <td>0.186502</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6917.0</td>\n",
       "      <td>831.0</td>\n",
       "      <td>6469.0</td>\n",
       "      <td>5138.0</td>\n",
       "      <td>7810.0</td>\n",
       "      <td>833.0</td>\n",
       "      <td>6488.0</td>\n",
       "      <td>5153.0</td>\n",
       "      <td>7833.0</td>\n",
       "      <td>7130.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3    4    5    6     7    8    9  ...  \\\n",
       "0  0 -0.341476  0.201175   20000.0  1.0  1.0  2.0  33.0  1.0  2.0  ...   \n",
       "1  0 -0.136859  0.199594   20000.0  2.0  2.0  2.0  35.0  0.0  0.0  ...   \n",
       "2  0 -0.284364  0.185736  230000.0  2.0  1.0  1.0  44.0  1.0 -1.0  ...   \n",
       "3  0 -0.040569  0.289360  100000.0  1.0  2.0  1.0  42.0  0.0  0.0  ...   \n",
       "4  0  0.079132  0.186502  150000.0  1.0  1.0  2.0  29.0 -2.0 -2.0  ...   \n",
       "\n",
       "        14       15       16       17       18      19      20      21  \\\n",
       "0  17399.0  19057.0  18453.0  19755.0  19288.0  2260.0     0.0  1600.0   \n",
       "1  19347.0  18600.0  19000.0  19000.0  20000.0     0.0  1000.0     0.0   \n",
       "2    949.0   2864.0    933.0      0.0      0.0  2873.0   933.0     0.0   \n",
       "3  99998.0  16138.0  17758.0  18774.0  20272.0  2000.0  2000.0  2000.0   \n",
       "4   6917.0    831.0   6469.0   5138.0   7810.0   833.0  6488.0  5153.0   \n",
       "\n",
       "       22      23  \n",
       "0     0.0   644.0  \n",
       "1  1000.0     0.0  \n",
       "2     0.0     0.0  \n",
       "3  2000.0  2000.0  \n",
       "4  7833.0  7130.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_full = pd.read_csv('test_data.csv', names = [str(x) for x in range(len(data.columns))])\n",
    "test_full.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81f1068b-3ae3-457f-a55e-276219f27580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label = test_full['0'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1d4fb36-053d-4296-a052-2feffff7bc2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating model with name: cc-training-job-1681712446\n",
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2023-04-17-06-25-06-115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................\n",
      "\u001b[34m[2023-04-17:06:29:22:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-04-17:06:29:22:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-04-17:06:29:22:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-04-17 06:29:22 +0000] [19] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2023-04-17 06:29:22 +0000] [19] [INFO] Listening at: unix:/tmp/gunicorn.sock (19)\u001b[0m\n",
      "\u001b[34m[2023-04-17 06:29:22 +0000] [19] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2023-04-17 06:29:22 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m[2023-04-17 06:29:22 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[35m[2023-04-17:06:29:22:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-04-17:06:29:22:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-04-17:06:29:22:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2023-04-17 06:29:22 +0000] [19] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[35m[2023-04-17 06:29:22 +0000] [19] [INFO] Listening at: unix:/tmp/gunicorn.sock (19)\u001b[0m\n",
      "\u001b[35m[2023-04-17 06:29:22 +0000] [19] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2023-04-17 06:29:22 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[35m[2023-04-17 06:29:22 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[34m[2023-04-17 06:29:22 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[34m[2023-04-17 06:29:22 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[35m[2023-04-17 06:29:22 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[35m[2023-04-17 06:29:22 +0000] [29] [INFO] Booting worker with pid: 29\u001b[0m\n",
      "\u001b[34m[2023-04-17:06:29:27:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Apr/2023:06:29:27 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2023-04-17:06:29:27:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Apr/2023:06:29:27 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Apr/2023:06:29:27 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2023-04-17:06:29:27:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-04-17:06:29:27:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Apr/2023:06:29:27 +0000] \"POST /invocations HTTP/1.1\" 200 118175 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Apr/2023:06:29:27 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2023-04-17:06:29:27:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-04-17:06:29:27:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Apr/2023:06:29:27 +0000] \"POST /invocations HTTP/1.1\" 200 118175 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2023-04-17T06:29:27.115:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "CPU times: user 549 ms, sys: 31 ms, total: 580 ms\n",
      "Wall time: 4min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sm_transformer = xgb.transformer(1, 'ml.m5.xlarge', accept = 'text/csv')\n",
    "\n",
    "# start a transform job\n",
    "sm_transformer.transform(test_data_path, split_type='Line', input_filter='$[1:]', content_type='text/csv')\n",
    "sm_transformer.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "013ddf33-7706-4a0a-baa9-9a716d4271ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>4460</td>\n",
       "      <td>815</td>\n",
       "      <td>5275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>243</td>\n",
       "      <td>482</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4703</td>\n",
       "      <td>1297</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1   All\n",
       "Actual                     \n",
       "0.0        4460   815  5275\n",
       "1.0         243   482   725\n",
       "All        4703  1297  6000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import io\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_csv_output_from_s3(s3uri, file_name):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket_name = parsed_url.netloc\n",
    "    prefix = parsed_url.path[1:]\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Object(bucket_name, '{}/{}'.format(prefix, file_name))\n",
    "    return obj.get()[\"Body\"].read().decode('utf-8')\n",
    "output = get_csv_output_from_s3(sm_transformer.output_path, 'test_data.csv.out')\n",
    "output_df = pd.read_csv(io.StringIO(output), sep=\",\", header=None)\n",
    "output_df.head(8)\n",
    "output_df['Predicted']=np.round(output_df.values)\n",
    "output_df['Label'] = label\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix = pd.crosstab(output_df['Predicted'], output_df['Label'], rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1e627e8-305a-4542-9713-9d03201ed8c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy = 0.7787999999999999\n",
      "Accuracy Score = 0.8236666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline Accuracy = {}\".format(1- np.unique(data['Label'], return_counts=True)[1][1]/(len(data['Label']))))\n",
    "print(\"Accuracy Score = {}\".format(accuracy_score(label, output_df['Predicted'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6593bf-37b5-403e-bf5a-2035381e3050",
   "metadata": {},
   "source": [
    "# Deploy as an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ab99109-62e0-4e9e-8cba-f82210bcb907",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created: arn:aws:sagemaker:us-east-1:477728513638:model/cc-training-job-1681712446\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker import RealTimePredictor\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "latest_training_job = sm_client.list_training_jobs(MaxResults=1,\n",
    "                                                SortBy='CreationTime',\n",
    "                                                SortOrder='Descending')\n",
    "\n",
    "training_job_name=TrainingJobName=latest_training_job['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "\n",
    "training_job_description = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "\n",
    "model_data = training_job_description['ModelArtifacts']['S3ModelArtifacts']\n",
    "container_uri = training_job_description['AlgorithmSpecification']['TrainingImage']\n",
    "\n",
    "# create a model.\n",
    "def create_model(role, model_name, container_uri, model_data):\n",
    "    return sm_client.create_model(\n",
    "        ModelName=model_name,\n",
    "        PrimaryContainer={\n",
    "        'Image': container_uri,\n",
    "        'ModelDataUrl': model_data,\n",
    "        },\n",
    "        ExecutionRoleArn=role)\n",
    "    \n",
    "\n",
    "try:\n",
    "    model = create_model(role, training_job_name, container_uri, model_data)\n",
    "except Exception as e:\n",
    "        sm_client.delete_model(ModelName=training_job_name)\n",
    "        model = create_model(role, training_job_name, container_uri, model_data)\n",
    "        \n",
    "\n",
    "print('Model created: '+model['ModelArn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf296dc9-bbfe-4fd2-9f3a-6420b5653764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_capture_upload_path = 's3://{}/{}/monitoring/datacapture'.format(rawbucket, prefix)\n",
    "data_capture_configuration = {\n",
    "    \"EnableCapture\": True,\n",
    "    \"InitialSamplingPercentage\": 100,\n",
    "    \"DestinationS3Uri\": s3_capture_upload_path,\n",
    "    \"CaptureOptions\": [\n",
    "        { \"CaptureMode\": \"Output\" },\n",
    "        { \"CaptureMode\": \"Input\" }\n",
    "    ],\n",
    "    \"CaptureContentTypeHeader\": {\n",
    "       \"CsvContentTypes\": [\"text/csv\"],\n",
    "       \"JsonContentTypes\": [\"application/json\"]}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff045c62-5a74-4dad-90b2-266ce0cb9b53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint configuration created: arn:aws:sagemaker:us-east-1:477728513638:endpoint-config/cc-training-job-1681712446\n"
     ]
    }
   ],
   "source": [
    "def create_endpoint_config(model_config, data_capture_config): \n",
    "    return sm_client.create_endpoint_config(\n",
    "                                                EndpointConfigName=model_config,\n",
    "                                                ProductionVariants=[\n",
    "                                                        {\n",
    "                                                            'VariantName': 'AllTraffic',\n",
    "                                                            'ModelName': model_config,\n",
    "                                                            'InitialInstanceCount': 1,\n",
    "                                                            'InstanceType': 'ml.m4.xlarge',\n",
    "                                                            'InitialVariantWeight': 1.0,\n",
    "                                                },\n",
    "                                                    \n",
    "                                                    ],\n",
    "                                                DataCaptureConfig=data_capture_config\n",
    "                                                )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    endpoint_config = create_endpoint_config(training_job_name, data_capture_configuration)\n",
    "except Exception as e:\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=endpoint)\n",
    "    endpoint_config = create_endpoint_config(training_job_name, data_capture_configuration)\n",
    "\n",
    "print('Endpoint configuration created: '+ endpoint_config['EndpointConfigArn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61c893cf-1f1e-43e1-9d6a-3471ace5eaf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint created: arn:aws:sagemaker:us-east-1:477728513638:endpoint/cc-training-job-1681712446\n"
     ]
    }
   ],
   "source": [
    "# Enable data capture, sampling 100% of the data for now. Next we deploy the endpoint in the correct VPC.\n",
    "\n",
    "endpoint_name = training_job_name\n",
    "def create_endpoint(endpoint_name, config_name):\n",
    "    return sm_client.create_endpoint(\n",
    "                                    EndpointName=endpoint_name,\n",
    "                                    EndpointConfigName=training_job_name\n",
    "                                )\n",
    "\n",
    "\n",
    "try:\n",
    "    endpoint = create_endpoint(endpoint_name, endpoint_config)\n",
    "except Exception as e:\n",
    "    sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint = create_endpoint(endpoint_name, endpoint_config)\n",
    "\n",
    "print('Endpoint created: '+ endpoint['EndpointArn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8793575-38d4-4570-8432-c27dfd6e8919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head -10 test_data.csv > test_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22b68f2c-8bff-460d-afdb-ce9e4ea274fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import RealTimePredictor\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, content_type = 'text/csv')\n",
    "\n",
    "with open('test_sample.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = predictor.predict(data=payload[2:])\n",
    "        sleep(0.5)\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91615e98-d500-4e22-800b-3cb7dbdf5aee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-modelmonitor/monitoring/datacapture/cc-training-job-1681712446/AllTraffic\n",
      "Found Capture Files:\n",
      "sagemaker-modelmonitor/monitoring/datacapture/cc-training-job-1681712446/AllTraffic/2023/04/17/06/33-36-309-7be904e3-1333-4c93-8d8a-d899f2e2436d.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sagemaker-modelmonitor/monitoring/datacapture/cc-training-job-1681712446/AllTraffic/2023/04/17/06/33-36-309-7be904e3-1333-4c93-8d8a-d899f2e2436d.jsonl'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the captured json files.\n",
    "data_capture_prefix = '{}/monitoring'.format(prefix)\n",
    "s3_client = boto3.Session().client('s3')\n",
    "current_endpoint_capture_prefix = '{}/datacapture/{}/AllTraffic'.format(data_capture_prefix, endpoint_name)\n",
    "print(current_endpoint_capture_prefix)\n",
    "result = s3_client.list_objects(Bucket=rawbucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))\n",
    "\n",
    "\n",
    "capture_files[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fabf0d49-adcf-4f7d-8c47-1c216fb6aa0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"data\": \"-0.34147611300851444,0.1932005252116958,50000.0,1.0,2.0,2.0,25.0,-1.0,3.0,2.0,0.0,0.0,0.0,10386.0,9993.0,9993.0,15300.0,0.0,0.0,200.0,5307.0,0.0,0.0\",\n",
      "      \"encoding\": \"CSV\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"observedContentType\": \"text/csv\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"data\": \"0.5108723044395447\",\n",
      "      \"encoding\": \"CSV\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"fdedc19b-02e4-45dc-8252-f1503c7a070c\",\n",
      "    \"inferenceTime\": \"2023-04-17T06:33:38Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# View contents of the captured file.\n",
    "def get_obj_body(bucket, obj_key):\n",
    "    return s3_client.get_object(Bucket=rawbucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(rawbucket, capture_files[0])\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[5]), indent = 2, sort_keys =True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d757635-0572-423d-8fd2-4fb152107472",
   "metadata": {},
   "source": [
    "# Monitor endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5377c52f-0a91-4e36-9232-f6e78021697e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline data uri: s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/cc-training-job-1681712446/baselining/data\n",
      "Baseline results uri: s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/cc-training-job-1681712446/baselining/results\n",
      "s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/train_headers\n"
     ]
    }
   ],
   "source": [
    "model_prefix = prefix + \"/\" + endpoint_name\n",
    "baseline_prefix = model_prefix + '/baselining'\n",
    "baseline_data_prefix = baseline_prefix + '/data'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = 's3://{}/{}'.format(rawbucket,baseline_data_prefix)\n",
    "baseline_results_uri = 's3://{}/{}'.format(rawbucket, baseline_results_prefix)\n",
    "train_data_header_location = \"s3://\" + rawbucket + '/' + prefix + '/train_headers'\n",
    "print('Baseline data uri: {}'.format(baseline_data_uri))\n",
    "print('Baseline results uri: {}'.format(baseline_results_uri))\n",
    "print(train_data_header_location)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f51775b-9be3-41e7-84bc-af7db5391234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2023-04-17-06-37-44-744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  baseline-suggestion-job-2023-04-17-06-37-44-744\n",
      "Inputs:  [{'InputName': 'baseline_dataset_input', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/train_headers/train_data_with_headers.csv', 'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'monitoring_output', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/cc-training-job-1681712446/baselining/results', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".........................\u001b[34m2023-04-17 06:41:47,412 - matplotlib.font_manager - INFO - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:47.945951: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:47.945983: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:49.544841: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:49.544871: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:49.544891: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-126-38.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:49.545167: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:51,179 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:477728513638:processing-job/baseline-suggestion-job-2023-04-17-06-37-44-744', 'ProcessingJobName': 'baseline-suggestion-job-2023-04-17-06-37-44-744', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/train_headers/train_data_with_headers.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/cc-training-job-1681712446/baselining/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::477728513638:role/CFN-SM-IM-Lambda-Catalog-SageMakerExecutionRole-1GMGL94UP3F1R', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:51,179 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:51,179 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:51,179 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:51,179 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:51,237 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:51,238 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:51,238 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:51,246 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:51,246 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:51,246 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:51,724 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.126.38\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yar\u001b[0m\n",
      "\u001b[34mn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_362\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:51,734 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:51,738 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-d7223b85-725d-4b7d-9c0b-bea3f1acf8a7\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,269 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,281 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,282 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,285 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,290 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,290 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,290 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,290 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,323 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,335 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,335 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,340 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,343 INFO blockmanagement.BlockManager: The block deletion will start around 2023 Apr 17 06:41:52\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,345 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,345 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,346 INFO util.GSet: 2.0% max memory 3.1 GB = 63.8 MB\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,346 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,382 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,386 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,386 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,386 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,386 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,386 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,386 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,386 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,386 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,386 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,386 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,386 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,413 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,413 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,413 INFO util.GSet: 1.0% max memory 3.1 GB = 31.9 MB\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,413 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,415 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,415 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,415 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,416 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,421 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,425 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,425 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,425 INFO util.GSet: 0.25% max memory 3.1 GB = 8.0 MB\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,425 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,464 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,465 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,465 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,468 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,468 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,470 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,470 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,470 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 979.8 KB\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,470 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,494 INFO namenode.FSImage: Allocated new BlockPoolId: BP-183572704-10.2.126.38-1681713712486\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,506 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,514 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,594 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,607 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,611 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.126.38\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:52,622 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:54,680 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:54,680 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:56,769 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:56,769 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:58,881 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-04-17 06:41:58,881 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:00,992 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:00,993 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:03,213 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:03,214 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:13,224 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:14,965 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:15,420 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:15,465 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:15,476 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:15,927 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:15,953 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:15,954 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:15,954 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:15,955 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:15,980 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11536, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:15,994 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:15,996 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:16,052 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:16,052 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:16,052 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:16,053 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:16,053 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:16,417 INFO util.Utils: Successfully started service 'sparkDriver' on port 37647.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:16,453 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:16,504 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:16,529 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:16,529 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:16,573 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:16,605 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-46e8d71e-347e-438f-a4c0-2bb32cde6618\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:16,625 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:16,681 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:16,730 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.2.126.38:37647/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1681713735922\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:17,358 INFO client.RMProxy: Connecting to ResourceManager at /10.2.126.38:8032\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:18,140 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:18,141 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:18,151 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15731 MB per container)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:18,152 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:18,152 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:18,153 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:18,161 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:18,261 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:20,004 INFO yarn.Client: Uploading resource file:/tmp/spark-8faaffd1-27d5-4eeb-9c48-f9b4e239de42/__spark_libs__3220546313468978598.zip -> hdfs://10.2.126.38/user/root/.sparkStaging/application_1681713718446_0001/__spark_libs__3220546313468978598.zip\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:21,771 INFO yarn.Client: Uploading resource file:/tmp/spark-8faaffd1-27d5-4eeb-9c48-f9b4e239de42/__spark_conf__5095206470671666426.zip -> hdfs://10.2.126.38/user/root/.sparkStaging/application_1681713718446_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:21,813 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:21,814 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:21,814 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:21,814 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:21,814 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:21,845 INFO yarn.Client: Submitting application application_1681713718446_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:22,055 INFO impl.YarnClientImpl: Submitted application application_1681713718446_0001\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:23,060 INFO yarn.Client: Application report for application_1681713718446_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:23,065 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Mon Apr 17 06:42:22 +0000 2023] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1681713741950\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1681713718446_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:24,075 INFO yarn.Client: Application report for application_1681713718446_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:25,079 INFO yarn.Client: Application report for application_1681713718446_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:26,086 INFO yarn.Client: Application report for application_1681713718446_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:27,090 INFO yarn.Client: Application report for application_1681713718446_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:27,735 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1681713718446_0001), /proxy/application_1681713718446_0001\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:28,097 INFO yarn.Client: Application report for application_1681713718446_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:28,097 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.126.38\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1681713741950\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1681713718446_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:28,099 INFO cluster.YarnClientSchedulerBackend: Application application_1681713718446_0001 has started running.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:28,113 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34517.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:28,117 INFO netty.NettyBlockTransferService: Server created on 10.2.126.38:34517\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:28,123 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:28,132 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.126.38, 34517, None)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:28,138 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.126.38:34517 with 1458.6 MiB RAM, BlockManagerId(driver, 10.2.126.38, 34517, None)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:28,143 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.126.38, 34517, None)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:28,145 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.126.38, 34517, None)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:28,343 INFO util.log: Logging initialized @14872ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:29,320 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:32,894 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.126.38:46856) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:33,086 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:46715 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 46715, None)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:47,214 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:47,321 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:47,355 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:47,357 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:48,317 INFO datasources.InMemoryFileIndex: It took 32 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:48,497 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:48,786 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:48,789 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.126.38:34517 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:48,793 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,181 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,183 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,187 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 3887463\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,245 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,266 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,267 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,268 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,269 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,275 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,332 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,339 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,339 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.126.38:34517 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,340 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,360 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,361 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,409 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4636 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:49,697 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:46715 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:50,527 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:46715 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:50,931 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1537 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:50,933 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:50,939 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.637 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:50,943 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:50,943 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:50,945 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.699779 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:51,161 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.2.126.38:34517 in memory (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:51,169 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:46715 in memory (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:51,206 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:46715 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:51,208 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.2.126.38:34517 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,329 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,331 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,333 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Label: string, PAY_AMT1: string, BILL_AMT1: string, LIMIT_BAL: string, SEX: string ... 22 more fields>\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,547 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,567 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,568 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.126.38:34517 (size: 39.1 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,569 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:100\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,584 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,632 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:100\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,634 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:100) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,634 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:100)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,634 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,665 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,669 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:100), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,721 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 19.7 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,724 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,725 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.126.38:34517 (size: 8.7 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,726 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,726 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:100) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,726 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,731 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:53,793 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:46715 (size: 8.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:54,799 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:46715 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:55,650 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:46715 (size: 3.2 MiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:55,793 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2065 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:55,793 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:55,794 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:100) finished in 2.121 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:55,795 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:55,795 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:55,795 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:100, took 2.162946 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,110 INFO codegen.CodeGenerator: Code generated in 238.463592 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,618 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,763 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,767 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,767 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,767 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,770 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,771 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,795 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 116.7 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,798 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,799 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.126.38:34517 (size: 35.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,800 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,802 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,802 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,809 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:56,830 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:46715 (size: 35.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,266 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1458 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,266 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,269 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.495 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,270 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,270 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,271 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,271 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,434 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,437 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,437 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,437 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,437 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,438 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,457 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 169.4 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,460 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 46.6 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,460 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.126.38:34517 (size: 46.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,461 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,462 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,462 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,465 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,486 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:46715 (size: 46.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,536 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.2.126.38:46856\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,922 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 458 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,924 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.479 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,925 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,926 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,926 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,927 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.492818 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:58,957 INFO codegen.CodeGenerator: Code generated in 21.543314 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:59,268 INFO codegen.CodeGenerator: Code generated in 35.389031 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:59,352 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:59,355 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:59,355 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:59,356 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:59,357 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:59,358 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:59,388 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 40.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:59,392 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:59,392 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.2.126.38:34517 (size: 17.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:59,394 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:59,394 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:59,395 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:59,397 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:42:59,428 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:46715 (size: 17.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:00,513 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 1117 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:00,514 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:00,519 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 1.159 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:00,520 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:00,520 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:00,520 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 1.166438 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:00,631 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.2.126.38:34517 in memory (size: 17.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:00,635 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:46715 in memory (size: 17.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:00,650 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.2.126.38:34517 in memory (size: 35.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:00,652 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:46715 in memory (size: 35.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:00,658 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.2.126.38:34517 in memory (size: 46.6 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:00,666 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:46715 in memory (size: 46.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:00,678 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.2.126.38:34517 in memory (size: 8.7 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:00,683 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:46715 in memory (size: 8.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,119 INFO codegen.CodeGenerator: Code generated in 118.845065 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,129 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,130 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,130 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,130 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,130 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,132 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,140 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 76.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,142 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,143 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.2.126.38:34517 (size: 24.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,144 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,145 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,146 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,147 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,164 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:46715 (size: 24.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,369 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 222 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,370 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,371 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.235 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,373 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,373 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,375 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,375 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,593 INFO codegen.CodeGenerator: Code generated in 88.829811 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,611 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,613 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,613 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,614 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,614 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,615 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,621 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.5 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,624 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,625 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.2.126.38:34517 (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,625 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,626 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,626 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,629 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,648 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:46715 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,656 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.126.38:46856\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,732 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 104 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,732 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,734 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.115 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,738 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,738 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,739 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.127297 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:01,857 INFO codegen.CodeGenerator: Code generated in 82.583083 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:02,019 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:02,025 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:02,026 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:02,026 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:02,026 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:02,027 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:02,031 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:02,047 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 32.7 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:02,061 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.7 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:02,062 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.2.126.38:34517 (size: 14.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:02,062 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:02,063 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:02,063 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:02,068 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:02,082 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:46715 (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,620 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1552 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,620 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,622 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.589 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,622 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,622 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,622 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,622 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,622 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,626 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,627 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,628 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.2.126.38:34517 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,628 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,629 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,629 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,631 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,641 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:46715 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,648 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.2.126.38:46856\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,680 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 50 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,680 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,681 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.058 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,682 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,682 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,682 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.662554 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,872 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,873 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,873 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,873 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,874 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,876 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,881 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 85.7 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,883 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,884 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.2.126.38:34517 (size: 27.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,884 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,885 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,885 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,887 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:03,899 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:46715 (size: 27.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,132 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 246 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,132 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,133 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.256 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,133 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,137 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,137 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,137 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,175 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,176 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,177 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,177 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,177 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,180 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,188 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 170.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,190 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 46.9 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,190 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.2.126.38:34517 (size: 46.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,191 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,191 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,191 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,192 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,202 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:46715 (size: 46.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,212 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.2.126.38:46856\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,323 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 131 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,323 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,324 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.143 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,324 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,324 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,324 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.148962 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,457 INFO codegen.CodeGenerator: Code generated in 17.309978 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,493 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,494 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,494 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,494 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,495 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,496 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,505 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 40.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,525 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,529 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.2.126.38:34517 (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,530 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,530 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:46715 in memory (size: 46.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,532 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.2.126.38:34517 in memory (size: 46.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,532 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,535 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,537 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,552 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:46715 (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,557 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.2.126.38:34517 in memory (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,558 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:46715 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,589 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.2.126.38:34517 in memory (size: 14.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,624 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:46715 in memory (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,676 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.2.126.38:34517 in memory (size: 27.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,684 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:46715 in memory (size: 27.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,700 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.2.126.38:34517 in memory (size: 24.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,709 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:46715 in memory (size: 24.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,745 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.2.126.38:34517 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:04,753 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:46715 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,181 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 645 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,181 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,182 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.685 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,182 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,183 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,183 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.689937 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,444 INFO codegen.CodeGenerator: Code generated in 66.52848 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,452 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,452 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,452 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,453 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,453 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,454 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,458 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 75.8 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,460 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,461 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.2.126.38:34517 (size: 24.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,462 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,462 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,462 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,464 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,483 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:46715 (size: 24.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,694 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 230 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,695 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.240 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,695 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,695 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,695 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,695 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,696 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,826 INFO codegen.CodeGenerator: Code generated in 73.873599 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,846 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,849 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,849 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,850 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,850 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,851 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,855 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,858 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,861 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.2.126.38:34517 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,862 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,863 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,864 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,865 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,878 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:46715 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:05,885 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.2.126.38:46856\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,074 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 209 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,075 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,076 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.223 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,078 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,078 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,078 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.231381 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,203 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,205 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,206 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,206 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,207 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,207 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,208 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,215 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 32.7 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,221 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 14.7 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,223 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.2.126.38:34517 (size: 14.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,224 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,227 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,228 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,230 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,242 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:46715 (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,495 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 266 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,496 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,501 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.292 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,502 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,503 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,503 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,503 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,504 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,514 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,517 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,518 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.2.126.38:34517 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,520 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,524 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,524 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,530 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,555 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:46715 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,563 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.2.126.38:46856\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,611 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 82 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,612 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,614 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.107 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,615 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,617 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:06,617 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.413240 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,232 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,232 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,233 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,233 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,234 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,235 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,247 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 85.7 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,250 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 27.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,251 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.2.126.38:34517 (size: 27.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,252 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,253 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,253 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,256 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,275 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:46715 (size: 27.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,538 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 283 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,538 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,539 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.302 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,539 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,539 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,539 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,539 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,605 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,608 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,609 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,609 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,610 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,613 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,625 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 170.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,628 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 46.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,629 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.2.126.38:34517 (size: 46.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,630 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,631 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,631 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,634 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,652 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:46715 (size: 46.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,665 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.2.126.38:46856\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,770 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 137 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,771 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,775 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.160 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,776 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,776 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:07,778 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.172456 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,079 INFO codegen.CodeGenerator: Code generated in 48.261403 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,152 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:46715 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,163 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.2.126.38:34517 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,166 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,168 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,168 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,168 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,169 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,170 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,181 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 40.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,183 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,183 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.2.126.38:34517 (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,184 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,185 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,185 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,187 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,204 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:46715 (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,216 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:46715 in memory (size: 46.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,229 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.2.126.38:34517 in memory (size: 46.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,337 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.2.126.38:34517 in memory (size: 14.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,372 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:46715 in memory (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,480 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:46715 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,490 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.2.126.38:34517 in memory (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,535 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.2.126.38:34517 in memory (size: 16.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,553 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:46715 in memory (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,674 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:46715 in memory (size: 27.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,685 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.2.126.38:34517 in memory (size: 27.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,712 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.2.126.38:34517 in memory (size: 24.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,715 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:46715 in memory (size: 24.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,991 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 805 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,991 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,992 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.819 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,994 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,995 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:08,996 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.829115 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,565 INFO codegen.CodeGenerator: Code generated in 196.298925 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,582 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,583 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,583 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,584 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,585 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,585 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,593 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 75.8 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,595 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,598 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.2.126.38:34517 (size: 24.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,599 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,602 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,603 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,605 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,628 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:46715 (size: 24.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,814 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 209 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,815 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,816 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.227 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,817 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,818 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,818 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,818 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,925 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,926 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,926 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,926 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,927 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,927 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,930 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 66.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,932 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,933 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.2.126.38:34517 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,934 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,935 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,935 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,937 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,952 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:46715 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,958 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.2.126.38:46856\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,968 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 31 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,969 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,969 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.041 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,970 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,970 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:09,971 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.045208 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,051 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,052 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,053 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,053 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,053 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,053 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,054 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,059 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 32.7 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,061 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 14.7 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,061 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.2.126.38:34517 (size: 14.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,062 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,063 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,063 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,065 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,082 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:46715 (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,159 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 94 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,159 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,160 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.105 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,160 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,160 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,160 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,160 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,161 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,163 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,165 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,166 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.2.126.38:34517 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,166 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,167 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,167 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,169 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,183 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:46715 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,192 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.2.126.38:46856\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,206 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 37 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,206 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,207 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.045 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,208 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,209 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,210 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.158318 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,431 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,432 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,432 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,433 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,433 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,434 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,439 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 85.7 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,442 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 28.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,442 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.2.126.38:34517 (size: 28.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,443 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,443 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,443 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,445 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,461 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:46715 (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,669 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 224 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,669 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,670 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.235 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,671 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,671 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,671 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,671 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,718 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,719 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,719 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,719 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,720 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,720 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,728 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 170.6 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,731 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 46.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,732 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.2.126.38:34517 (size: 46.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,733 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,733 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,734 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,736 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,751 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:46715 (size: 46.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,763 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.2.126.38:46856\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,858 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 123 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,859 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,859 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.137 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,860 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,860 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:10,860 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.142189 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:11,025 INFO codegen.CodeGenerator: Code generated in 19.488509 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:11,061 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:11,063 INFO scheduler.DAGScheduler: Got job 22 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:11,063 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:11,063 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:11,064 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:11,065 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:11,077 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 40.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:11,079 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:11,079 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.2.126.38:34517 (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:11,080 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:11,081 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:11,081 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:11,082 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:11,095 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:46715 (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,140 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 1058 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,141 INFO scheduler.DAGScheduler: ResultStage 32 (treeReduce at KLLRunner.scala:107) finished in 1.075 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,141 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,141 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,141 INFO cluster.YarnScheduler: Killing all running tasks in stage 32: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,142 INFO scheduler.DAGScheduler: Job 22 finished: treeReduce at KLLRunner.scala:107, took 1.080173 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,374 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.2.126.38:34517 in memory (size: 46.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,375 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:46715 in memory (size: 46.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,417 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.2.126.38:34517 in memory (size: 16.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,431 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:46715 in memory (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,476 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.2.126.38:34517 in memory (size: 16.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,485 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:46715 in memory (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,544 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.2.126.38:34517 in memory (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,563 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:46715 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,614 INFO codegen.CodeGenerator: Code generated in 82.730235 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,616 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.2.126.38:34517 in memory (size: 14.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,623 INFO scheduler.DAGScheduler: Registering RDD 139 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,623 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,624 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,624 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,625 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:46715 in memory (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,626 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,626 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,639 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 75.8 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,641 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,646 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.2.126.38:34517 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,649 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:46715 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,650 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.2.126.38:34517 (size: 23.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,652 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,655 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,656 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,654 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.2.126.38:34517 in memory (size: 24.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,658 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,659 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:46715 in memory (size: 24.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,670 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.2.126.38:34517 in memory (size: 28.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,675 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:46715 in memory (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,676 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:46715 (size: 23.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,848 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 190 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,848 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,849 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326) finished in 0.222 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,849 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,850 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,850 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,850 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,994 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,995 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,997 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,997 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,997 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:12,999 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,001 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 66.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,005 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,007 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.2.126.38:34517 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,007 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,008 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,008 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,010 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,022 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:46715 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,028 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.2.126.38:46856\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,036 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 26 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,036 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,037 INFO scheduler.DAGScheduler: ResultStage 35 (collect at AnalysisRunner.scala:326) finished in 0.037 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,038 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,038 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,039 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.044484 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,394 INFO scheduler.DAGScheduler: Registering RDD 147 (collect at AnalysisRunner.scala:326) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,395 INFO scheduler.DAGScheduler: Got map stage job 25 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,395 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 36 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,395 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,395 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,396 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[147] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,401 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 75.6 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,407 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,408 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.2.126.38:34517 (size: 25.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,410 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,410 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[147] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,410 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,412 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,436 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:46715 (size: 25.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,844 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 432 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,845 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,846 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (collect at AnalysisRunner.scala:326) finished in 0.448 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,848 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,849 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,849 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,849 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,901 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,903 INFO scheduler.DAGScheduler: Got job 26 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,903 INFO scheduler.DAGScheduler: Final stage: ResultStage 38 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,903 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,903 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,903 INFO scheduler.DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[150] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,911 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 144.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,917 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 40.8 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,919 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.2.126.38:34517 (size: 40.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,921 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,921 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[150] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,922 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,925 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 29) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,940 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:46715 (size: 40.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:13,956 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.2.126.38:46856\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,054 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 29) in 130 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,054 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,055 INFO scheduler.DAGScheduler: ResultStage 38 (collect at AnalysisRunner.scala:326) finished in 0.151 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,058 INFO scheduler.DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,058 INFO cluster.YarnScheduler: Killing all running tasks in stage 38: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,059 INFO scheduler.DAGScheduler: Job 26 finished: collect at AnalysisRunner.scala:326, took 0.158173 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,095 INFO codegen.CodeGenerator: Code generated in 32.432596 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,280 INFO codegen.CodeGenerator: Code generated in 28.035296 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,328 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,330 INFO scheduler.DAGScheduler: Got job 27 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,330 INFO scheduler.DAGScheduler: Final stage: ResultStage 39 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,331 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,331 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,332 INFO scheduler.DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[160] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,338 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 39.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,340 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,341 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.2.126.38:34517 (size: 16.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,342 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,342 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[160] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,342 INFO cluster.YarnScheduler: Adding task set 39.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,344 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 39.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,355 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:46715 (size: 16.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,772 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 39.0 (TID 30) in 428 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,772 INFO cluster.YarnScheduler: Removed TaskSet 39.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,773 INFO scheduler.DAGScheduler: ResultStage 39 (treeReduce at KLLRunner.scala:107) finished in 0.440 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,774 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,774 INFO cluster.YarnScheduler: Killing all running tasks in stage 39: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,775 INFO scheduler.DAGScheduler: Job 27 finished: treeReduce at KLLRunner.scala:107, took 0.445372 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,963 INFO codegen.CodeGenerator: Code generated in 62.770994 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,973 INFO scheduler.DAGScheduler: Registering RDD 165 (collect at AnalysisRunner.scala:326) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,974 INFO scheduler.DAGScheduler: Got map stage job 28 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,974 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 40 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,974 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,975 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,975 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[165] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,979 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 65.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,981 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 21.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,982 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.2.126.38:34517 (size: 21.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,982 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,983 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[165] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,983 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,984 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:14,996 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:46715 (size: 21.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,128 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 144 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,128 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,129 INFO scheduler.DAGScheduler: ShuffleMapStage 40 (collect at AnalysisRunner.scala:326) finished in 0.153 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,129 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,130 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,130 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,130 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,292 INFO codegen.CodeGenerator: Code generated in 86.931363 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,311 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,314 INFO scheduler.DAGScheduler: Got job 29 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,314 INFO scheduler.DAGScheduler: Final stage: ResultStage 42 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,315 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,315 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,315 INFO scheduler.DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[168] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,318 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 55.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,320 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,320 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.2.126.38:34517 (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,321 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,322 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[168] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,322 INFO cluster.YarnScheduler: Adding task set 42.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,324 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 32) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,348 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-1:46715 (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,353 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.2.126.38:46856\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,417 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 32) in 93 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,417 INFO cluster.YarnScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,418 INFO scheduler.DAGScheduler: ResultStage 42 (collect at AnalysisRunner.scala:326) finished in 0.101 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,418 INFO scheduler.DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,419 INFO cluster.YarnScheduler: Killing all running tasks in stage 42: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,421 INFO scheduler.DAGScheduler: Job 29 finished: collect at AnalysisRunner.scala:326, took 0.107395 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,520 INFO codegen.CodeGenerator: Code generated in 90.875656 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,673 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on algo-1:46715 in memory (size: 40.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,708 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.2.126.38:34517 in memory (size: 40.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,714 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:46715 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,727 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.2.126.38:34517 in memory (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,741 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.2.126.38:34517 in memory (size: 25.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,744 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:46715 in memory (size: 25.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,752 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.2.126.38:34517 in memory (size: 21.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,755 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on algo-1:46715 in memory (size: 21.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,766 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.2.126.38:34517 in memory (size: 16.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,767 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on algo-1:46715 in memory (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,792 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.2.126.38:34517 in memory (size: 23.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,793 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:46715 in memory (size: 23.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,801 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on algo-1:46715 in memory (size: 16.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:15,802 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.2.126.38:34517 in memory (size: 16.8 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,073 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,119 INFO codegen.CodeGenerator: Code generated in 17.042075 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,127 INFO scheduler.DAGScheduler: Registering RDD 173 (count at StatsGenerator.scala:66) as input to shuffle 13\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,127 INFO scheduler.DAGScheduler: Got map stage job 30 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,127 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 43 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,128 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,128 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,129 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 43 (MapPartitionsRDD[173] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,132 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 24.7 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,134 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,135 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.2.126.38:34517 (size: 10.9 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,136 INFO spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,136 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[173] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,136 INFO cluster.YarnScheduler: Adding task set 43.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,138 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 43.0 (TID 33) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,155 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on algo-1:46715 (size: 10.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,214 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 43.0 (TID 33) in 76 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,214 INFO cluster.YarnScheduler: Removed TaskSet 43.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,216 INFO scheduler.DAGScheduler: ShuffleMapStage 43 (count at StatsGenerator.scala:66) finished in 0.086 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,216 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,216 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,217 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,217 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,254 INFO codegen.CodeGenerator: Code generated in 22.014212 ms\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,278 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,284 INFO scheduler.DAGScheduler: Got job 31 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,284 INFO scheduler.DAGScheduler: Final stage: ResultStage 45 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,284 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,285 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,285 INFO scheduler.DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[176] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,288 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 11.1 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,289 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,290 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.2.126.38:34517 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,291 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,291 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[176] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,292 INFO cluster.YarnScheduler: Adding task set 45.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,293 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 34) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,305 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on algo-1:46715 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,309 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.2.126.38:46856\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,356 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 34) in 63 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,357 INFO cluster.YarnScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,357 INFO scheduler.DAGScheduler: ResultStage 45 (count at StatsGenerator.scala:66) finished in 0.071 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,357 INFO scheduler.DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,358 INFO cluster.YarnScheduler: Killing all running tasks in stage 45: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:16,358 INFO scheduler.DAGScheduler: Job 31 finished: count at StatsGenerator.scala:66, took 0.075075 s\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,111 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,172 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,211 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,211 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,218 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,261 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,345 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,345 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,367 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,371 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,426 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,426 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,426 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,459 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,466 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-8faaffd1-27d5-4eeb-9c48-f9b4e239de42\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,487 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-7e91c4e1-5af9-40c1-859b-d9f368172059\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,579 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2023-04-17 06:43:17,579 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7ff88e4b1050>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600)\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=os.path.join(train_data_header_location, 'train_data_with_headers.csv'),\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eef39d58-905f-4011-a3b0-5c889b0a645b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Files:\n",
      "sagemaker-modelmonitor/cc-training-job-1681712446/baselining/results/constraints.json\n",
      " sagemaker-modelmonitor/cc-training-job-1681712446/baselining/results/statistics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>numerical_statistics.common.num_present</th>\n",
       "      <th>numerical_statistics.common.num_missing</th>\n",
       "      <th>numerical_statistics.mean</th>\n",
       "      <th>numerical_statistics.sum</th>\n",
       "      <th>numerical_statistics.std_dev</th>\n",
       "      <th>numerical_statistics.min</th>\n",
       "      <th>numerical_statistics.max</th>\n",
       "      <th>numerical_statistics.distribution.kll.buckets</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.c</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.k</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Label</td>\n",
       "      <td>Integral</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.224583e-01</td>\n",
       "      <td>5.339000e+03</td>\n",
       "      <td>0.415897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PAY_AMT1</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.965234e-16</td>\n",
       "      <td>4.716560e-12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.341476</td>\n",
       "      <td>5.223016e+01</td>\n",
       "      <td>[{'lower_bound': -0.34147611300851444, 'upper_...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-0.16093173468294658, -0.2812946535666585, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BILL_AMT1</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.378091e-01</td>\n",
       "      <td>5.707419e+03</td>\n",
       "      <td>0.080585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.0999999...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.19552189076210494, 0.2369590330493186, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIMIT_BAL</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.677310e+05</td>\n",
       "      <td>4.025544e+09</td>\n",
       "      <td>129479.698677</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>8.000000e+05</td>\n",
       "      <td>[{'lower_bound': 10000.0, 'upper_bound': 89000...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[30000.0, 120000.0, 200000.0, 130000.0, 28000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SEX</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.601167e+00</td>\n",
       "      <td>3.842800e+04</td>\n",
       "      <td>0.489658</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>[{'lower_bound': 1.0, 'upper_bound': 1.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.851083e+00</td>\n",
       "      <td>4.442600e+04</td>\n",
       "      <td>0.788030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.6, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MARRIAGE</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.553125e+00</td>\n",
       "      <td>3.727500e+04</td>\n",
       "      <td>0.521227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.3, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AGE</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>3.545954e+01</td>\n",
       "      <td>8.510290e+05</td>\n",
       "      <td>9.191179</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.900000e+01</td>\n",
       "      <td>[{'lower_bound': 21.0, 'upper_bound': 26.8, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[26.0, 27.0, 58.0, 29.0, 49.0, 23.0, 26.0, 29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PAY_0</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.316667e-02</td>\n",
       "      <td>-3.160000e+02</td>\n",
       "      <td>1.127531</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-1.0, 0.0, 1.0, 0.0, -2.0, -1.0, -1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PAY_2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.276667e-01</td>\n",
       "      <td>-3.064000e+03</td>\n",
       "      <td>1.199702</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.1, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 2.0, 0.0, -2.0, -1.0, 0.0, 0.0, 5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PAY_3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.595833e-01</td>\n",
       "      <td>-3.830000e+03</td>\n",
       "      <td>1.198833</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, -1.0, 2.0, 0.0, -2.0, -1.0, 0.0, 0.0, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PAY_4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.157500e-01</td>\n",
       "      <td>-5.178000e+03</td>\n",
       "      <td>1.168026</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, -1.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PAY_5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.625833e-01</td>\n",
       "      <td>-6.302000e+03</td>\n",
       "      <td>1.130582</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-1.0, -2.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PAY_6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.858750e-01</td>\n",
       "      <td>-6.861000e+03</td>\n",
       "      <td>1.148760</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-1.0, -2.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BILL_AMT2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.929893e+04</td>\n",
       "      <td>1.183174e+09</td>\n",
       "      <td>70960.574221</td>\n",
       "      <td>-69777.000000</td>\n",
       "      <td>7.439700e+05</td>\n",
       "      <td>[{'lower_bound': -69777.0, 'upper_bound': 1159...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[14899.0, 15000.0, 206084.0, 67664.0, 6163.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BILL_AMT3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.717089e+04</td>\n",
       "      <td>1.132101e+09</td>\n",
       "      <td>69505.298901</td>\n",
       "      <td>-157264.000000</td>\n",
       "      <td>1.664089e+06</td>\n",
       "      <td>[{'lower_bound': -157264.0, 'upper_bound': 248...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[16790.0, 100.0, 187747.0, 66258.0, -54.0, 44...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BILL_AMT4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.325871e+04</td>\n",
       "      <td>1.038209e+09</td>\n",
       "      <td>64153.274849</td>\n",
       "      <td>-170000.000000</td>\n",
       "      <td>7.068640e+05</td>\n",
       "      <td>[{'lower_bound': -170000.0, 'upper_bound': -82...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[12400.0, 0.0, 159746.0, 62697.0, -54.0, 4536...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BILL_AMT5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.036543e+04</td>\n",
       "      <td>9.687702e+08</td>\n",
       "      <td>60673.598646</td>\n",
       "      <td>-81334.000000</td>\n",
       "      <td>8.235400e+05</td>\n",
       "      <td>[{'lower_bound': -81334.0, 'upper_bound': 9153...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[780.0, 0.0, 140632.0, 48210.0, -54.0, 46135....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BILL_AMT6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>3.891905e+04</td>\n",
       "      <td>9.340573e+08</td>\n",
       "      <td>59424.681250</td>\n",
       "      <td>-339603.000000</td>\n",
       "      <td>6.999440e+05</td>\n",
       "      <td>[{'lower_bound': -339603.0, 'upper_bound': -23...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 143092.0, 46255.0, -54.0, 46029.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PAY_AMT2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>5.926893e+03</td>\n",
       "      <td>1.422454e+08</td>\n",
       "      <td>23436.624063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.684259e+06</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 168425.9,...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[3000.0, 100.0, 0.0, 3000.0, 0.0, 45361.0, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PAY_AMT3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>5.186248e+03</td>\n",
       "      <td>1.244700e+08</td>\n",
       "      <td>17186.818924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.890430e+05</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 88904.3, ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[6000.0, 0.0, 6014.0, 6000.0, 0.0, 1800.0, 65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PAY_AMT4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.830179e+03</td>\n",
       "      <td>1.159243e+08</td>\n",
       "      <td>15482.205461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.210000e+05</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 62100.0, ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[780.0, 0.0, 5000.0, 2000.0, 0.0, 1503.0, 706...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PAY_AMT5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.797411e+03</td>\n",
       "      <td>1.151379e+08</td>\n",
       "      <td>15166.388154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.880710e+05</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 38807.1, ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 6000.0, 2000.0, 0.0, 1735.0, 954.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PAY_AMT6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>5.219489e+03</td>\n",
       "      <td>1.252677e+08</td>\n",
       "      <td>17606.700346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.271430e+05</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 52714.3, ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 5000.0, 2000.0, 0.0, 1724.0, 700.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name inferred_type  numerical_statistics.common.num_present  \\\n",
       "0       Label      Integral                                    24000   \n",
       "1    PAY_AMT1    Fractional                                    24000   \n",
       "2   BILL_AMT1    Fractional                                    24000   \n",
       "3   LIMIT_BAL    Fractional                                    24000   \n",
       "4         SEX    Fractional                                    24000   \n",
       "5   EDUCATION    Fractional                                    24000   \n",
       "6    MARRIAGE    Fractional                                    24000   \n",
       "7         AGE    Fractional                                    24000   \n",
       "8       PAY_0    Fractional                                    24000   \n",
       "9       PAY_2    Fractional                                    24000   \n",
       "10      PAY_3    Fractional                                    24000   \n",
       "11      PAY_4    Fractional                                    24000   \n",
       "12      PAY_5    Fractional                                    24000   \n",
       "13      PAY_6    Fractional                                    24000   \n",
       "14  BILL_AMT2    Fractional                                    24000   \n",
       "15  BILL_AMT3    Fractional                                    24000   \n",
       "16  BILL_AMT4    Fractional                                    24000   \n",
       "17  BILL_AMT5    Fractional                                    24000   \n",
       "18  BILL_AMT6    Fractional                                    24000   \n",
       "19   PAY_AMT2    Fractional                                    24000   \n",
       "20   PAY_AMT3    Fractional                                    24000   \n",
       "21   PAY_AMT4    Fractional                                    24000   \n",
       "22   PAY_AMT5    Fractional                                    24000   \n",
       "23   PAY_AMT6    Fractional                                    24000   \n",
       "\n",
       "    numerical_statistics.common.num_missing  numerical_statistics.mean  \\\n",
       "0                                         0               2.224583e-01   \n",
       "1                                         0               1.965234e-16   \n",
       "2                                         0               2.378091e-01   \n",
       "3                                         0               1.677310e+05   \n",
       "4                                         0               1.601167e+00   \n",
       "5                                         0               1.851083e+00   \n",
       "6                                         0               1.553125e+00   \n",
       "7                                         0               3.545954e+01   \n",
       "8                                         0              -1.316667e-02   \n",
       "9                                         0              -1.276667e-01   \n",
       "10                                        0              -1.595833e-01   \n",
       "11                                        0              -2.157500e-01   \n",
       "12                                        0              -2.625833e-01   \n",
       "13                                        0              -2.858750e-01   \n",
       "14                                        0               4.929893e+04   \n",
       "15                                        0               4.717089e+04   \n",
       "16                                        0               4.325871e+04   \n",
       "17                                        0               4.036543e+04   \n",
       "18                                        0               3.891905e+04   \n",
       "19                                        0               5.926893e+03   \n",
       "20                                        0               5.186248e+03   \n",
       "21                                        0               4.830179e+03   \n",
       "22                                        0               4.797411e+03   \n",
       "23                                        0               5.219489e+03   \n",
       "\n",
       "    numerical_statistics.sum  numerical_statistics.std_dev  \\\n",
       "0               5.339000e+03                      0.415897   \n",
       "1               4.716560e-12                      1.000000   \n",
       "2               5.707419e+03                      0.080585   \n",
       "3               4.025544e+09                 129479.698677   \n",
       "4               3.842800e+04                      0.489658   \n",
       "5               4.442600e+04                      0.788030   \n",
       "6               3.727500e+04                      0.521227   \n",
       "7               8.510290e+05                      9.191179   \n",
       "8              -3.160000e+02                      1.127531   \n",
       "9              -3.064000e+03                      1.199702   \n",
       "10             -3.830000e+03                      1.198833   \n",
       "11             -5.178000e+03                      1.168026   \n",
       "12             -6.302000e+03                      1.130582   \n",
       "13             -6.861000e+03                      1.148760   \n",
       "14              1.183174e+09                  70960.574221   \n",
       "15              1.132101e+09                  69505.298901   \n",
       "16              1.038209e+09                  64153.274849   \n",
       "17              9.687702e+08                  60673.598646   \n",
       "18              9.340573e+08                  59424.681250   \n",
       "19              1.422454e+08                  23436.624063   \n",
       "20              1.244700e+08                  17186.818924   \n",
       "21              1.159243e+08                  15482.205461   \n",
       "22              1.151379e+08                  15166.388154   \n",
       "23              1.252677e+08                  17606.700346   \n",
       "\n",
       "    numerical_statistics.min  numerical_statistics.max  \\\n",
       "0                   0.000000              1.000000e+00   \n",
       "1                  -0.341476              5.223016e+01   \n",
       "2                   0.000000              1.000000e+00   \n",
       "3               10000.000000              8.000000e+05   \n",
       "4                   1.000000              2.000000e+00   \n",
       "5                   0.000000              6.000000e+00   \n",
       "6                   0.000000              3.000000e+00   \n",
       "7                  21.000000              7.900000e+01   \n",
       "8                  -2.000000              8.000000e+00   \n",
       "9                  -2.000000              7.000000e+00   \n",
       "10                 -2.000000              8.000000e+00   \n",
       "11                 -2.000000              8.000000e+00   \n",
       "12                 -2.000000              8.000000e+00   \n",
       "13                 -2.000000              8.000000e+00   \n",
       "14             -69777.000000              7.439700e+05   \n",
       "15            -157264.000000              1.664089e+06   \n",
       "16            -170000.000000              7.068640e+05   \n",
       "17             -81334.000000              8.235400e+05   \n",
       "18            -339603.000000              6.999440e+05   \n",
       "19                  0.000000              1.684259e+06   \n",
       "20                  0.000000              8.890430e+05   \n",
       "21                  0.000000              6.210000e+05   \n",
       "22                  0.000000              3.880710e+05   \n",
       "23                  0.000000              5.271430e+05   \n",
       "\n",
       "        numerical_statistics.distribution.kll.buckets  \\\n",
       "0   [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "1   [{'lower_bound': -0.34147611300851444, 'upper_...   \n",
       "2   [{'lower_bound': 0.0, 'upper_bound': 0.0999999...   \n",
       "3   [{'lower_bound': 10000.0, 'upper_bound': 89000...   \n",
       "4   [{'lower_bound': 1.0, 'upper_bound': 1.1, 'cou...   \n",
       "5   [{'lower_bound': 0.0, 'upper_bound': 0.6, 'cou...   \n",
       "6   [{'lower_bound': 0.0, 'upper_bound': 0.3, 'cou...   \n",
       "7   [{'lower_bound': 21.0, 'upper_bound': 26.8, 'c...   \n",
       "8   [{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...   \n",
       "9   [{'lower_bound': -2.0, 'upper_bound': -1.1, 'c...   \n",
       "10  [{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...   \n",
       "11  [{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...   \n",
       "12  [{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...   \n",
       "13  [{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...   \n",
       "14  [{'lower_bound': -69777.0, 'upper_bound': 1159...   \n",
       "15  [{'lower_bound': -157264.0, 'upper_bound': 248...   \n",
       "16  [{'lower_bound': -170000.0, 'upper_bound': -82...   \n",
       "17  [{'lower_bound': -81334.0, 'upper_bound': 9153...   \n",
       "18  [{'lower_bound': -339603.0, 'upper_bound': -23...   \n",
       "19  [{'lower_bound': 0.0, 'upper_bound': 168425.9,...   \n",
       "20  [{'lower_bound': 0.0, 'upper_bound': 88904.3, ...   \n",
       "21  [{'lower_bound': 0.0, 'upper_bound': 62100.0, ...   \n",
       "22  [{'lower_bound': 0.0, 'upper_bound': 38807.1, ...   \n",
       "23  [{'lower_bound': 0.0, 'upper_bound': 52714.3, ...   \n",
       "\n",
       "    numerical_statistics.distribution.kll.sketch.parameters.c  \\\n",
       "0                                                0.64           \n",
       "1                                                0.64           \n",
       "2                                                0.64           \n",
       "3                                                0.64           \n",
       "4                                                0.64           \n",
       "5                                                0.64           \n",
       "6                                                0.64           \n",
       "7                                                0.64           \n",
       "8                                                0.64           \n",
       "9                                                0.64           \n",
       "10                                               0.64           \n",
       "11                                               0.64           \n",
       "12                                               0.64           \n",
       "13                                               0.64           \n",
       "14                                               0.64           \n",
       "15                                               0.64           \n",
       "16                                               0.64           \n",
       "17                                               0.64           \n",
       "18                                               0.64           \n",
       "19                                               0.64           \n",
       "20                                               0.64           \n",
       "21                                               0.64           \n",
       "22                                               0.64           \n",
       "23                                               0.64           \n",
       "\n",
       "    numerical_statistics.distribution.kll.sketch.parameters.k  \\\n",
       "0                                              2048.0           \n",
       "1                                              2048.0           \n",
       "2                                              2048.0           \n",
       "3                                              2048.0           \n",
       "4                                              2048.0           \n",
       "5                                              2048.0           \n",
       "6                                              2048.0           \n",
       "7                                              2048.0           \n",
       "8                                              2048.0           \n",
       "9                                              2048.0           \n",
       "10                                             2048.0           \n",
       "11                                             2048.0           \n",
       "12                                             2048.0           \n",
       "13                                             2048.0           \n",
       "14                                             2048.0           \n",
       "15                                             2048.0           \n",
       "16                                             2048.0           \n",
       "17                                             2048.0           \n",
       "18                                             2048.0           \n",
       "19                                             2048.0           \n",
       "20                                             2048.0           \n",
       "21                                             2048.0           \n",
       "22                                             2048.0           \n",
       "23                                             2048.0           \n",
       "\n",
       "    numerical_statistics.distribution.kll.sketch.data  \n",
       "0   [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1   [[-0.16093173468294658, -0.2812946535666585, 0...  \n",
       "2   [[0.19552189076210494, 0.2369590330493186, 0.4...  \n",
       "3   [[30000.0, 120000.0, 200000.0, 130000.0, 28000...  \n",
       "4   [[1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0,...  \n",
       "5   [[2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0,...  \n",
       "6   [[2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0,...  \n",
       "7   [[26.0, 27.0, 58.0, 29.0, 49.0, 23.0, 26.0, 29...  \n",
       "8   [[-1.0, 0.0, 1.0, 0.0, -2.0, -1.0, -1.0, 0.0, ...  \n",
       "9   [[0.0, 0.0, 2.0, 0.0, -2.0, -1.0, 0.0, 0.0, 5....  \n",
       "10  [[0.0, -1.0, 2.0, 0.0, -2.0, -1.0, 0.0, 0.0, 4...  \n",
       "11  [[0.0, -1.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 3....  \n",
       "12  [[-1.0, -2.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 2...  \n",
       "13  [[-1.0, -2.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0...  \n",
       "14  [[14899.0, 15000.0, 206084.0, 67664.0, 6163.0,...  \n",
       "15  [[16790.0, 100.0, 187747.0, 66258.0, -54.0, 44...  \n",
       "16  [[12400.0, 0.0, 159746.0, 62697.0, -54.0, 4536...  \n",
       "17  [[780.0, 0.0, 140632.0, 48210.0, -54.0, 46135....  \n",
       "18  [[0.0, 0.0, 143092.0, 46255.0, -54.0, 46029.0,...  \n",
       "19  [[3000.0, 100.0, 0.0, 3000.0, 0.0, 45361.0, 12...  \n",
       "20  [[6000.0, 0.0, 6014.0, 6000.0, 0.0, 1800.0, 65...  \n",
       "21  [[780.0, 0.0, 5000.0, 2000.0, 0.0, 1503.0, 706...  \n",
       "22  [[0.0, 0.0, 6000.0, 2000.0, 0.0, 1735.0, 954.0...  \n",
       "23  [[0.0, 0.0, 5000.0, 2000.0, 0.0, 1724.0, 700.0...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=rawbucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81b4c567-f46e-49d5-96d2-193ab14f1142",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>completeness</th>\n",
       "      <th>num_constraints.is_non_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Label</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PAY_AMT1</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BILL_AMT1</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIMIT_BAL</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SEX</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MARRIAGE</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AGE</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PAY_0</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PAY_2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PAY_3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PAY_4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PAY_5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PAY_6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BILL_AMT2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BILL_AMT3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BILL_AMT4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BILL_AMT5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BILL_AMT6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PAY_AMT2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PAY_AMT3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PAY_AMT4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PAY_AMT5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PAY_AMT6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name inferred_type  completeness  num_constraints.is_non_negative\n",
       "0       Label      Integral           1.0                             True\n",
       "1    PAY_AMT1    Fractional           1.0                            False\n",
       "2   BILL_AMT1    Fractional           1.0                             True\n",
       "3   LIMIT_BAL    Fractional           1.0                             True\n",
       "4         SEX    Fractional           1.0                             True\n",
       "5   EDUCATION    Fractional           1.0                             True\n",
       "6    MARRIAGE    Fractional           1.0                             True\n",
       "7         AGE    Fractional           1.0                             True\n",
       "8       PAY_0    Fractional           1.0                            False\n",
       "9       PAY_2    Fractional           1.0                            False\n",
       "10      PAY_3    Fractional           1.0                            False\n",
       "11      PAY_4    Fractional           1.0                            False\n",
       "12      PAY_5    Fractional           1.0                            False\n",
       "13      PAY_6    Fractional           1.0                            False\n",
       "14  BILL_AMT2    Fractional           1.0                            False\n",
       "15  BILL_AMT3    Fractional           1.0                            False\n",
       "16  BILL_AMT4    Fractional           1.0                            False\n",
       "17  BILL_AMT5    Fractional           1.0                            False\n",
       "18  BILL_AMT6    Fractional           1.0                            False\n",
       "19   PAY_AMT2    Fractional           1.0                             True\n",
       "20   PAY_AMT3    Fractional           1.0                             True\n",
       "21   PAY_AMT4    Fractional           1.0                             True\n",
       "22   PAY_AMT5    Fractional           1.0                             True\n",
       "23   PAY_AMT6    Fractional           1.0                             True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b829747f-703c-4fbc-a215-21182e604bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating monitoring schedule name Built-train-deploy-model-monitor-schedule-2023-04-17-06-44-03.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/reports\n",
      "\n",
      "Creating Monitoring Schedule with name: Built-train-deploy-model-monitor-schedule-2023-04-17-06-44-03\n"
     ]
    }
   ],
   "source": [
    "reports_prefix = '{}/reports'.format(prefix)\n",
    "s3_report_path = 's3://{}/{}'.format(rawbucket,reports_prefix)\n",
    "print(s3_report_path)\n",
    "\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "mon_schedule_name = 'Built-train-deploy-model-monitor-schedule-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=predictor.endpoint,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcffeeb6-0e44-4a89-a254-a5b005a7e93e",
   "metadata": {},
   "source": [
    "# Test Model Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4aa1e4a5-b9e8-4af4-aa05-ccf5877bc104",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.341476</td>\n",
       "      <td>0.201175</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17399.0</td>\n",
       "      <td>19057.0</td>\n",
       "      <td>18453.0</td>\n",
       "      <td>19755.0</td>\n",
       "      <td>19288.0</td>\n",
       "      <td>2260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.136859</td>\n",
       "      <td>0.199594</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19347.0</td>\n",
       "      <td>18600.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.284364</td>\n",
       "      <td>0.185736</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2864.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2873.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.040569</td>\n",
       "      <td>0.289360</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>99998.0</td>\n",
       "      <td>16138.0</td>\n",
       "      <td>17758.0</td>\n",
       "      <td>18774.0</td>\n",
       "      <td>20272.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.079132</td>\n",
       "      <td>0.186502</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6917.0</td>\n",
       "      <td>831.0</td>\n",
       "      <td>6469.0</td>\n",
       "      <td>5138.0</td>\n",
       "      <td>7810.0</td>\n",
       "      <td>833.0</td>\n",
       "      <td>6488.0</td>\n",
       "      <td>5153.0</td>\n",
       "      <td>7833.0</td>\n",
       "      <td>7130.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  PAY_AMT1  BILL_AMT1  LIMIT_BAL  SEX  EDUCATION  MARRIAGE   AGE  \\\n",
       "0      0 -0.341476   0.201175    20000.0  1.0        1.0       2.0  33.0   \n",
       "1      0 -0.136859   0.199594    20000.0  2.0        2.0       2.0  35.0   \n",
       "2      0 -0.284364   0.185736   230000.0  2.0        1.0       1.0  44.0   \n",
       "3      0 -0.040569   0.289360   100000.0  1.0        2.0       1.0  42.0   \n",
       "4      0  0.079132   0.186502   150000.0  1.0        1.0       2.0  29.0   \n",
       "\n",
       "   PAY_0  PAY_2  ...  BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  BILL_AMT6  \\\n",
       "0    1.0    2.0  ...    17399.0    19057.0    18453.0    19755.0    19288.0   \n",
       "1    0.0    0.0  ...    19347.0    18600.0    19000.0    19000.0    20000.0   \n",
       "2    1.0   -1.0  ...      949.0     2864.0      933.0        0.0        0.0   \n",
       "3    0.0    0.0  ...    99998.0    16138.0    17758.0    18774.0    20272.0   \n",
       "4   -2.0   -2.0  ...     6917.0      831.0     6469.0     5138.0     7810.0   \n",
       "\n",
       "   PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \n",
       "0    2260.0       0.0    1600.0       0.0     644.0  \n",
       "1       0.0    1000.0       0.0    1000.0       0.0  \n",
       "2    2873.0     933.0       0.0       0.0       0.0  \n",
       "3    2000.0    2000.0    2000.0    2000.0    2000.0  \n",
       "4     833.0    6488.0    5153.0    7833.0    7130.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLS = data.columns\n",
    "test_full = pd.read_csv('test_data.csv', names = ['Label'] +['PAY_AMT1','BILL_AMT1'] + list(COLS[1:])[:11] + list(COLS[1:])[12:17] + list(COLS[1:])[18:]\n",
    ")\n",
    "test_full.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1ed8366-3132-41e8-a1a4-1fc74c959036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "faketestdata = test_full\n",
    "faketestdata['EDUCATION'] = -faketestdata['EDUCATION'].astype(float)\n",
    "faketestdata['BILL_AMT2']= (faketestdata['BILL_AMT2']//10).astype(float)\n",
    "faketestdata['AGE']= (faketestdata['AGE']-10).astype(float)\n",
    "\n",
    "faketestdata.head()\n",
    "faketestdata.drop(columns=['Label']).to_csv('test-data-input-cols.csv', index = None, header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "203c3fb0-8e76-4cce-94cf-4b561b1fb7e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-39-09ef44b3458a>\", line 17, in invoke_endpoint_forever\n",
      "    endpoint_arn = runtime_client.describe_endpoint(EndpointName=endpoint)['EndpointArn']\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/botocore/client.py\", line 877, in __getattr__\n",
      "    f\"'{self.__class__.__name__}' object has no attribute '{item}'\"\n",
      "AttributeError: 'SageMakerRuntime' object has no attribute 'describe_endpoint'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "# (just repeating code from above for convenience/ able to run this section independently)\n",
    "def invoke_endpoint(ep_name, file_name, runtime_client):\n",
    "    with open(file_name, 'r') as f:\n",
    "        for row in f:\n",
    "            payload = row.rstrip('\\n')\n",
    "            response = runtime_client.invoke_endpoint(EndpointName=ep_name,\n",
    "                                          ContentType='text/csv', \n",
    "                                          Body=payload)\n",
    "            time.sleep(1)\n",
    "            \n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        endpoint_arn = runtime_client.describe_endpoint(EndpointName=endpoint)['EndpointArn']\n",
    "        invoke_endpoint(endpoint_arn, 'test-data-input-cols.csv', runtime_client)\n",
    "        \n",
    "thread = Thread(target = invoke_endpoint_forever)\n",
    "thread.start()\n",
    "# Note that you need to stop the kernel to stop the invocations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e94733b3-93f0-4cc2-bf93-cd85d9b68906",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule status: Scheduled\n"
     ]
    }
   ],
   "source": [
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fef25be6-50b9-4d63-be10-1a52a1f47a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No executions found for schedule. monitoring_schedule_name: Built-train-deploy-model-monitor-schedule-2023-04-17-06-44-03\n",
      "We created ahourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer.\n",
      "We will have to wait till we hit the hour...\n",
      "Waiting for the 1st execution to happen...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-c1bd6f4ac220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmon_executions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Waiting for the 1st execution to happen...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmon_executions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_default_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_executions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mon_executions = my_default_monitor.list_executions()\n",
    "print(\"We created ahourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer.\\nWe will have to wait till we hit the hour...\")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(600)\n",
    "    mon_executions = my_default_monitor.list_executions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b28e52-f8e5-4eee-950c-15165afe6db0",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4b5dc9c-823c-41bc-a563-806df86212cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting Monitoring Schedule with name: Built-train-deploy-model-monitor-schedule-2023-04-17-06-44-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%sh` not found.\n"
     ]
    }
   ],
   "source": [
    "my_default_monitor.delete_monitoring_schedule()\n",
    "time.sleep(10) # actually wait for the deletion\n",
    "sm.delete_endpoint(EndpointName = endpoint_name)\n",
    "%%sh aws s3 rm --recursive s3://sagemaker-us-east-2-ACCOUNT_NUMBER/sagemaker-modelmonitor/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb87411-f941-462b-a5c8-620a962db5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
