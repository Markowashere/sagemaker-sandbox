{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32032a5-c5e5-4ecc-8992-5c16dfd08c69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version is good\n",
      "Region = us-east-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import sys\n",
    "import IPython\n",
    "\n",
    "if int(sagemaker.__version__.split('.')[0]) == 2:\n",
    "    print(\"Installing previous SageMaker Version and restarting the kernel\")\n",
    "    !{sys.executable} -m pip install sagemaker==1.72.0\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "else:\n",
    "    print(\"Version is good\")\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.session.Session().region_name\n",
    "print(\"Region = {}\".format(region))\n",
    "sm = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eabfda4-4c68-4e57-b007-1ba05531095e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.7/site-packages (0.1.43)\n",
      "Requirement already satisfied: boto3>=1.16.27 in /opt/conda/lib/python3.7/site-packages (from sagemaker-experiments) (1.26.111)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.111 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.29.111)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.111->boto3>=1.16.27->sagemaker-experiments) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.111->boto3>=1.16.27->sagemaker-experiments) (1.26.15)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.111->boto3>=1.16.27->sagemaker-experiments) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import sleep, gmtime, strftime\n",
    "import json\n",
    "import time\n",
    "\n",
    "#experiments\n",
    "!pip install sagemaker-experiments \n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2091ba2-0bd0-4405-8680-3f251f5ae255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rawbucket= sess.default_bucket() # Alternatively you can use our custom bucket here. \n",
    "\n",
    "prefix = 'sagemaker-modelmonitor' # use this prefix to store all files pertaining to this workshop.\n",
    "\n",
    "dataprefix = prefix + '/data'\n",
    "traindataprefix = prefix + '/train_data'\n",
    "testdataprefix = prefix + '/test_data'\n",
    "testdatanolabelprefix = prefix + '/test_data_no_label'\n",
    "trainheaderprefix = prefix + '/train_headers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604a8b8b-19f8-4bcd-a6c0-cd320e5f9945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-19 09:02:21--  https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5539328 (5.3M) [application/x-httpd-php]\n",
      "Saving to: ‘default of credit card clients.xls’\n",
      "\n",
      "default of credit c 100%[===================>]   5.28M  9.31MB/s    in 0.6s    \n",
      "\n",
      "2023-04-19 09:02:22 (9.31 MB/s) - ‘default of credit card clients.xls’ saved [5539328/5539328]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0      20000    2          2         1   24      2      2     -1     -1   \n",
       "1     120000    2          2         2   26     -1      2      0      0   \n",
       "2      90000    2          2         2   34      0      0      0      0   \n",
       "3      50000    2          2         1   37      0      0      0      0   \n",
       "4      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0     -2  ...          0          0          0         0       689         0   \n",
       "1      0  ...       3272       3455       3261         0      1000      1000   \n",
       "2      0  ...      14331      14948      15549      1518      1500      1000   \n",
       "3      0  ...      28314      28959      29547      2000      2019      1200   \n",
       "4      0  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "0         0         0         0                           1  \n",
       "1      1000         0      2000                           1  \n",
       "2      1000      1000      5000                           0  \n",
       "3      1100      1069      1000                           0  \n",
       "4      9000       689       679                           0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\n",
    "data = pd.read_excel('default of credit card clients.xls', header=1)\n",
    "data = data.drop(columns = ['ID'])\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e6b67a-2a0b-4825-a4c9-597f5b40590e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2682</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13559</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49291</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>35835</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  \\\n",
       "0      1      20000    2          2         1   24      2      2     -1   \n",
       "1      1     120000    2          2         2   26     -1      2      0   \n",
       "2      0      90000    2          2         2   34      0      0      0   \n",
       "3      0      50000    2          2         1   37      0      0      0   \n",
       "4      0      50000    1          2         1   57     -1      0     -1   \n",
       "\n",
       "   PAY_4  ...  BILL_AMT3  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
       "0     -1  ...        689          0          0          0         0       689   \n",
       "1      0  ...       2682       3272       3455       3261         0      1000   \n",
       "2      0  ...      13559      14331      14948      15549      1518      1500   \n",
       "3      0  ...      49291      28314      28959      29547      2000      2019   \n",
       "4      0  ...      35835      20940      19146      19131      2000     36681   \n",
       "\n",
       "   PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \n",
       "0         0         0         0         0  \n",
       "1      1000      1000         0      2000  \n",
       "2      1000      1000      1000      5000  \n",
       "3      1200      1100      1069      1000  \n",
       "4     10000      9000       689       679  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rename(columns={\"default payment next month\": \"Label\"}, inplace=True)\n",
    "lbl = data.Label\n",
    "data = pd.concat([lbl, data.drop(columns=['Label'])], axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab8e7d34-d4a5-4155-bc59-c84c5c106523",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/data\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('rawdata/rawdata.csv'):\n",
    "    !mkdir rawdata\n",
    "    data.to_csv('rawdata/rawdata.csv', index=None)\n",
    "else:\n",
    "    pass\n",
    "# Upload the raw dataset\n",
    "raw_data_location = sess.upload_data('rawdata', bucket=rawbucket, key_prefix=dataprefix)\n",
    "print(raw_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a3f862-3c78-4490-8974-0e02a539a6a1",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8444f0b0-e95b-4542-a0f1-7ec2d8652814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.c4.xlarge',\n",
    "                                     instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59bb791e-1ba6-4a1b-a5b4-80d955ae50ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-test-split-ratio', type=float, default=0.3)\n",
    "    parser.add_argument('--random-split', type=int, default=0)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print('Received arguments {}'.format(args))\n",
    "\n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', 'rawdata.csv')\n",
    "    \n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    df.sample(frac=1)\n",
    "    \n",
    "    COLS = df.columns\n",
    "    newcolorder = ['PAY_AMT1','BILL_AMT1'] + list(COLS[1:])[:11] + list(COLS[1:])[12:17] + list(COLS[1:])[18:]\n",
    "    \n",
    "    split_ratio = args.train_test_split_ratio\n",
    "    random_state=args.random_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('Label', axis=1), df['Label'], \n",
    "                                                        test_size=split_ratio, random_state=random_state)\n",
    "    \n",
    "    preprocess = make_column_transformer(\n",
    "        (['PAY_AMT1'], StandardScaler()),\n",
    "        (['BILL_AMT1'], MinMaxScaler()),\n",
    "    remainder='passthrough')\n",
    "    \n",
    "    print('Running preprocessing and feature engineering transformations')\n",
    "    train_features = pd.DataFrame(preprocess.fit_transform(X_train), columns = newcolorder)\n",
    "    test_features = pd.DataFrame(preprocess.transform(X_test), columns = newcolorder)\n",
    "    \n",
    "    # concat to ensure Label column is the first column in dataframe\n",
    "    train_full = pd.concat([pd.DataFrame(y_train.values, columns=['Label']), train_features], axis=1)\n",
    "    test_full = pd.concat([pd.DataFrame(y_test.values, columns=['Label']), test_features], axis=1)\n",
    "    \n",
    "    print('Train data shape after preprocessing: {}'.format(train_features.shape))\n",
    "    print('Test data shape after preprocessing: {}'.format(test_features.shape))\n",
    "    \n",
    "    train_features_headers_output_path = os.path.join('/opt/ml/processing/train_headers', 'train_data_with_headers.csv')\n",
    "    \n",
    "    train_features_output_path = os.path.join('/opt/ml/processing/train', 'train_data.csv')\n",
    "    \n",
    "    test_features_output_path = os.path.join('/opt/ml/processing/test', 'test_data.csv')\n",
    "    \n",
    "    print('Saving training features to {}'.format(train_features_output_path))\n",
    "    train_full.to_csv(train_features_output_path, header=False, index=False)\n",
    "    print(\"Complete\")\n",
    "    \n",
    "    print(\"Save training data with headers to {}\".format(train_features_headers_output_path))\n",
    "    train_full.to_csv(train_features_headers_output_path, index=False)\n",
    "                 \n",
    "    print('Saving test features to {}'.format(test_features_output_path))\n",
    "    test_full.to_csv(test_features_output_path, header=False, index=False)\n",
    "    print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f498f3e-9ccf-4df6-a740-d740f972cf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/code/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "# Copy the preprocessing code over to the s3 bucket\n",
    "codeprefix = prefix + '/code'\n",
    "codeupload = sess.upload_data('preprocessing.py', bucket=rawbucket, key_prefix=codeprefix)\n",
    "print(codeupload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0448f598-16a6-45c7-9609-ffabf5dd3f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data location = sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/train_data\n",
      "Test data location = sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/test_data\n"
     ]
    }
   ],
   "source": [
    "train_data_location = rawbucket + '/' + traindataprefix\n",
    "test_data_location = rawbucket+'/'+testdataprefix\n",
    "print(\"Training data location = {}\".format(train_data_location))\n",
    "print(\"Test data location = {}\".format(test_data_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7fae014-adc2-4b2e-b1ab-4400b58137e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sagemaker-scikit-learn-2023-04-19-09-02-26-349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2023-04-19-09-02-26-349\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/data', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/train_data', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/test_data', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'train_data_headers', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/train_headers', 'LocalPath': '/opt/ml/processing/train_headers', 'S3UploadMode': 'EndOfJob'}}]\n",
      "...............................\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\u001b[0m\n",
      "\u001b[34mReceived arguments Namespace(random_split=0, train_test_split_ratio=0.2)\u001b[0m\n",
      "\u001b[34mReading input data from /opt/ml/processing/input/rawdata.csv\u001b[0m\n",
      "\u001b[34mRunning preprocessing and feature engineering transformations\u001b[0m\n",
      "\u001b[34mTrain data shape after preprocessing: (24000, 23)\u001b[0m\n",
      "\u001b[34mTest data shape after preprocessing: (6000, 23)\u001b[0m\n",
      "\u001b[34mSaving training features to /opt/ml/processing/train/train_data.csv\u001b[0m\n",
      "\u001b[34mComplete\u001b[0m\n",
      "\u001b[34mSave training data with headers to /opt/ml/processing/train_headers/train_data_with_headers.csv\u001b[0m\n",
      "\u001b[34mSaving test features to /opt/ml/processing/test/test_data.csv\u001b[0m\n",
      "\u001b[34mComplete\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(code=codeupload,\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=raw_data_location,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/train',\n",
    "                               destination='s3://' + train_data_location),\n",
    "                               ProcessingOutput(output_name='test_data',\n",
    "                                                source='/opt/ml/processing/test',\n",
    "                                               destination=\"s3://\"+test_data_location),\n",
    "                               ProcessingOutput(output_name='train_data_headers',\n",
    "                                                source='/opt/ml/processing/train_headers',\n",
    "                                               destination=\"s3://\" + rawbucket + '/' + prefix + '/train_headers')],\n",
    "                      arguments=['--train-test-split-ratio', '0.2']\n",
    "                     )\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train_data':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'test_data':\n",
    "        preprocessed_test_data = output['S3Output']['S3Uri']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31d8f9-f522-4e73-a038-6b99a9235f9e",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "229ca7a7-bbde-46c6-8048-1103e724b095",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7f181e9b02d0>,experiment_name='Build-train-deploy-1681895408',description='Predict credit card default from payments data',tags=None,experiment_arn='arn:aws:sagemaker:us-east-1:477728513638:experiment/build-train-deploy-1681895408',response_metadata={'RequestId': '2b5e9ee5-5134-4b26-9667-768e129fad16', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '2b5e9ee5-5134-4b26-9667-768e129fad16', 'content-type': 'application/x-amz-json-1.1', 'content-length': '101', 'date': 'Wed, 19 Apr 2023 09:10:07 GMT'}, 'RetryAttempts': 0})\n"
     ]
    }
   ],
   "source": [
    "# Create a SageMaker Experiment\n",
    "cc_experiment = Experiment.create(\n",
    "    experiment_name=f\"Build-train-deploy-{int(time.time())}\", \n",
    "    description=\"Predict credit card default from payments data\", \n",
    "    sagemaker_boto_client=sm)\n",
    "print(cc_experiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc4c3fd-85e4-40ef-9da1-259c88492542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start Tracking parameters used in the Pre-processing pipeline.\n",
    "with Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n",
    "    tracker.log_parameters({\n",
    "        \"train_test_split_ratio\": 0.2,\n",
    "        \"random_state\":0\n",
    "    })\n",
    "    # we can log the s3 uri to the dataset we just uploaded\n",
    "    tracker.log_input(name=\"ccdefault-raw-dataset\", media_type=\"s3/uri\", value=raw_data_location)\n",
    "    tracker.log_input(name=\"ccdefault-train-dataset\", media_type=\"s3/uri\", value=train_data_location)\n",
    "    tracker.log_input(name=\"ccdefault-test-dataset\", media_type=\"s3/uri\", value=test_data_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "757eebbc-cc85-4af1-a4b4-0c00420efcd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.amazon.amazon_estimator:'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker.estimator:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating training-job with name: cc-training-job-1681895412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-19 09:10:12 Starting - Starting the training job...\n",
      "2023-04-19 09:10:39 Starting - Preparing the instances for training......\n",
      "2023-04-19 09:11:43 Downloading - Downloading input data......\n",
      "2023-04-19 09:12:14 Training - Downloading the training image...\n",
      "2023-04-19 09:12:59 Training - Training image download completed. Training in progress...\u001b[34m[2023-04-19 09:13:16.450 ip-10-2-124-250.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 24000 rows\u001b[0m\n",
      "\u001b[34m[09:13:16] 24000x23 matrix with 552000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.17854\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.17746\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.17704\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.17717\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.17621\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.17608\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.17504\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.17467\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.17513\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.17475\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.17408\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.17354\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.17363\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.17313\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.17325\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.17296\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.17292\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.17317\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.17250\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.17200\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.17175\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.17142\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.17154\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.17171\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.17112\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.17033\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.17046\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.17029\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.17029\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.17029\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.17021\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.16996\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.16971\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.16929\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.16917\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.16908\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.16904\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.16900\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.16867\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.16846\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.16825\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.16817\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.16817\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.16800\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.16804\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.16804\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.16792\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.16737\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.16717\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.16654\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.16667\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.16642\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.16629\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.16542\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.16517\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.16529\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.16537\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.16537\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.16537\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.16529\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.16521\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.16500\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.16500\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.16487\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.16408\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.16400\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.16387\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.16387\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.16387\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.16375\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.16367\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.16333\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.16337\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.16317\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.16296\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.16287\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.16308\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.16300\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.16300\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.16271\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.16246\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.16237\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.16221\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.16171\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.16179\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.16096\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.16079\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.16067\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.16071\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.16042\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.16017\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.16000\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.16029\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.16012\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.16025\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.15975\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.15983\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.15967\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.15937\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.15887\u001b[0m\n",
      "\n",
      "2023-04-19 09:13:37 Uploading - Uploading generated training model\n",
      "2023-04-19 09:13:37 Completed - Training job completed\n",
      "Training seconds: 113\n",
      "Billable seconds: 113\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', '1.0-1')\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://' + train_data_location, content_type='csv')\n",
    "preprocessing_trial_component = tracker.trial_component\n",
    "\n",
    "trial_name = f\"cc-default-training-job-{int(time.time())}\"\n",
    "cc_trial = Trial.create(\n",
    "        trial_name=trial_name, \n",
    "            experiment_name=cc_experiment.experiment_name,\n",
    "        sagemaker_boto_client=sm\n",
    "    )\n",
    "\n",
    "cc_trial.add_trial_component(preprocessing_trial_component)\n",
    "cc_training_job_name = \"cc-training-job-{}\".format(int(time.time()))\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    train_max_run=86400,\n",
    "                                    output_path='s3://{}/{}/models'.format(rawbucket, prefix),\n",
    "                                    sagemaker_session=sess) # set to true for distributed training\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        verbosity=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit(inputs = {'train':s3_input_train},\n",
    "       job_name=cc_training_job_name,\n",
    "        experiment_config={\n",
    "            \"TrialName\": cc_trial.trial_name, #log training job in Trials for lineage\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        },\n",
    "        wait=True,\n",
    "    )\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f7caf-6141-41be-ae11-50c9a5dae47c",
   "metadata": {},
   "source": [
    "# Deploy for offline inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6457c50-c44b-4328-ba7d-8a4d52d83355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/test_data/test_data.csv to ./test_data.csv\n"
     ]
    }
   ],
   "source": [
    "test_data_path = 's3://' + test_data_location + '/test_data.csv'\n",
    "! aws s3 cp $test_data_path .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "834b298b-7fe4-4d70-8c04-150a52f0466b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.341476</td>\n",
       "      <td>0.201175</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17399.0</td>\n",
       "      <td>19057.0</td>\n",
       "      <td>18453.0</td>\n",
       "      <td>19755.0</td>\n",
       "      <td>19288.0</td>\n",
       "      <td>2260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.136859</td>\n",
       "      <td>0.199594</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19347.0</td>\n",
       "      <td>18600.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.284364</td>\n",
       "      <td>0.185736</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2864.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2873.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.040569</td>\n",
       "      <td>0.289360</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>99998.0</td>\n",
       "      <td>16138.0</td>\n",
       "      <td>17758.0</td>\n",
       "      <td>18774.0</td>\n",
       "      <td>20272.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.079132</td>\n",
       "      <td>0.186502</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6917.0</td>\n",
       "      <td>831.0</td>\n",
       "      <td>6469.0</td>\n",
       "      <td>5138.0</td>\n",
       "      <td>7810.0</td>\n",
       "      <td>833.0</td>\n",
       "      <td>6488.0</td>\n",
       "      <td>5153.0</td>\n",
       "      <td>7833.0</td>\n",
       "      <td>7130.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3    4    5    6     7    8    9  ...  \\\n",
       "0  0 -0.341476  0.201175   20000.0  1.0  1.0  2.0  33.0  1.0  2.0  ...   \n",
       "1  0 -0.136859  0.199594   20000.0  2.0  2.0  2.0  35.0  0.0  0.0  ...   \n",
       "2  0 -0.284364  0.185736  230000.0  2.0  1.0  1.0  44.0  1.0 -1.0  ...   \n",
       "3  0 -0.040569  0.289360  100000.0  1.0  2.0  1.0  42.0  0.0  0.0  ...   \n",
       "4  0  0.079132  0.186502  150000.0  1.0  1.0  2.0  29.0 -2.0 -2.0  ...   \n",
       "\n",
       "        14       15       16       17       18      19      20      21  \\\n",
       "0  17399.0  19057.0  18453.0  19755.0  19288.0  2260.0     0.0  1600.0   \n",
       "1  19347.0  18600.0  19000.0  19000.0  20000.0     0.0  1000.0     0.0   \n",
       "2    949.0   2864.0    933.0      0.0      0.0  2873.0   933.0     0.0   \n",
       "3  99998.0  16138.0  17758.0  18774.0  20272.0  2000.0  2000.0  2000.0   \n",
       "4   6917.0    831.0   6469.0   5138.0   7810.0   833.0  6488.0  5153.0   \n",
       "\n",
       "       22      23  \n",
       "0     0.0   644.0  \n",
       "1  1000.0     0.0  \n",
       "2     0.0     0.0  \n",
       "3  2000.0  2000.0  \n",
       "4  7833.0  7130.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_full = pd.read_csv('test_data.csv', names = [str(x) for x in range(len(data.columns))])\n",
    "test_full.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81f1068b-3ae3-457f-a55e-276219f27580",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label = test_full['0'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1d4fb36-053d-4296-a052-2feffff7bc2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating model with name: cc-training-job-1681895412\n",
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2023-04-19-09-14-01-347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "\u001b[34m[2023-04-19:09:18:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-04-19:09:18:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-04-19:09:18:44:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[35m[2023-04-19:09:18:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-04-19:09:18:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-04-19:09:18:44:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-04-19 09:18:44 +0000] [19] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2023-04-19 09:18:44 +0000] [19] [INFO] Listening at: unix:/tmp/gunicorn.sock (19)\u001b[0m\n",
      "\u001b[34m[2023-04-19 09:18:44 +0000] [19] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2023-04-19 09:18:44 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m[2023-04-19 09:18:44 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[34m[2023-04-19 09:18:44 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[34m[2023-04-19 09:18:44 +0000] [32] [INFO] Booting worker with pid: 32\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2023-04-19 09:18:44 +0000] [19] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[35m[2023-04-19 09:18:44 +0000] [19] [INFO] Listening at: unix:/tmp/gunicorn.sock (19)\u001b[0m\n",
      "\u001b[35m[2023-04-19 09:18:44 +0000] [19] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2023-04-19 09:18:44 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[35m[2023-04-19 09:18:44 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[35m[2023-04-19 09:18:44 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[35m[2023-04-19 09:18:44 +0000] [32] [INFO] Booting worker with pid: 32\u001b[0m\n",
      "\u001b[34m[2023-04-19:09:18:49:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [19/Apr/2023:09:18:49 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2023-04-19:09:18:49:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [19/Apr/2023:09:18:49 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2023-04-19:09:18:49:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-04-19:09:18:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [19/Apr/2023:09:18:49 +0000] \"POST /invocations HTTP/1.1\" 200 118175 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2023-04-19:09:18:49:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [19/Apr/2023:09:18:49 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2023-04-19:09:18:49:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [19/Apr/2023:09:18:49 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2023-04-19:09:18:49:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-04-19:09:18:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [19/Apr/2023:09:18:49 +0000] \"POST /invocations HTTP/1.1\" 200 118175 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2023-04-19T09:18:49.252:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "CPU times: user 629 ms, sys: 11.9 ms, total: 641 ms\n",
      "Wall time: 5min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sm_transformer = xgb.transformer(1, 'ml.m5.xlarge', accept = 'text/csv')\n",
    "\n",
    "# start a transform job\n",
    "sm_transformer.transform(test_data_path, split_type='Line', input_filter='$[1:]', content_type='text/csv')\n",
    "sm_transformer.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "013ddf33-7706-4a0a-baa9-9a716d4271ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>4460</td>\n",
       "      <td>815</td>\n",
       "      <td>5275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>243</td>\n",
       "      <td>482</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4703</td>\n",
       "      <td>1297</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0     1   All\n",
       "Actual                     \n",
       "0.0        4460   815  5275\n",
       "1.0         243   482   725\n",
       "All        4703  1297  6000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import io\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_csv_output_from_s3(s3uri, file_name):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket_name = parsed_url.netloc\n",
    "    prefix = parsed_url.path[1:]\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Object(bucket_name, '{}/{}'.format(prefix, file_name))\n",
    "    return obj.get()[\"Body\"].read().decode('utf-8')\n",
    "output = get_csv_output_from_s3(sm_transformer.output_path, 'test_data.csv.out')\n",
    "output_df = pd.read_csv(io.StringIO(output), sep=\",\", header=None)\n",
    "output_df.head(8)\n",
    "output_df['Predicted']=np.round(output_df.values)\n",
    "output_df['Label'] = label\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix = pd.crosstab(output_df['Predicted'], output_df['Label'], rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1e627e8-305a-4542-9713-9d03201ed8c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy = 0.7787999999999999\n",
      "Accuracy Score = 0.8236666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline Accuracy = {}\".format(1- np.unique(data['Label'], return_counts=True)[1][1]/(len(data['Label']))))\n",
    "print(\"Accuracy Score = {}\".format(accuracy_score(label, output_df['Predicted'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6593bf-37b5-403e-bf5a-2035381e3050",
   "metadata": {},
   "source": [
    "# Deploy as an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ab99109-62e0-4e9e-8cba-f82210bcb907",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created: arn:aws:sagemaker:us-east-1:477728513638:model/cc-training-job-1681895412\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker import RealTimePredictor\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "latest_training_job = sm_client.list_training_jobs(MaxResults=1,\n",
    "                                                SortBy='CreationTime',\n",
    "                                                SortOrder='Descending')\n",
    "\n",
    "training_job_name=TrainingJobName=latest_training_job['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "\n",
    "training_job_description = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "\n",
    "model_data = training_job_description['ModelArtifacts']['S3ModelArtifacts']\n",
    "container_uri = training_job_description['AlgorithmSpecification']['TrainingImage']\n",
    "\n",
    "# create a model.\n",
    "def create_model(role, model_name, container_uri, model_data):\n",
    "    return sm_client.create_model(\n",
    "        ModelName=model_name,\n",
    "        PrimaryContainer={\n",
    "        'Image': container_uri,\n",
    "        'ModelDataUrl': model_data,\n",
    "        },\n",
    "        ExecutionRoleArn=role)\n",
    "    \n",
    "\n",
    "try:\n",
    "    model = create_model(role, training_job_name, container_uri, model_data)\n",
    "except Exception as e:\n",
    "        sm_client.delete_model(ModelName=training_job_name)\n",
    "        model = create_model(role, training_job_name, container_uri, model_data)\n",
    "        \n",
    "\n",
    "print('Model created: '+model['ModelArn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf296dc9-bbfe-4fd2-9f3a-6420b5653764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_capture_upload_path = 's3://{}/{}/monitoring/datacapture'.format(rawbucket, prefix)\n",
    "data_capture_configuration = {\n",
    "    \"EnableCapture\": True,\n",
    "    \"InitialSamplingPercentage\": 100,\n",
    "    \"DestinationS3Uri\": s3_capture_upload_path,\n",
    "    \"CaptureOptions\": [\n",
    "        { \"CaptureMode\": \"Output\" },\n",
    "        { \"CaptureMode\": \"Input\" }\n",
    "    ],\n",
    "    \"CaptureContentTypeHeader\": {\n",
    "       \"CsvContentTypes\": [\"text/csv\"],\n",
    "       \"JsonContentTypes\": [\"application/json\"]}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff045c62-5a74-4dad-90b2-266ce0cb9b53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint configuration created: arn:aws:sagemaker:us-east-1:477728513638:endpoint-config/cc-training-job-1681895412\n"
     ]
    }
   ],
   "source": [
    "def create_endpoint_config(model_config, data_capture_config): \n",
    "    return sm_client.create_endpoint_config(\n",
    "                                                EndpointConfigName=model_config,\n",
    "                                                ProductionVariants=[\n",
    "                                                        {\n",
    "                                                            'VariantName': 'AllTraffic',\n",
    "                                                            'ModelName': model_config,\n",
    "                                                            'InitialInstanceCount': 1,\n",
    "                                                            'InstanceType': 'ml.m4.xlarge',\n",
    "                                                            'InitialVariantWeight': 1.0,\n",
    "                                                },\n",
    "                                                    \n",
    "                                                    ],\n",
    "                                                DataCaptureConfig=data_capture_config\n",
    "                                                )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    endpoint_config = create_endpoint_config(training_job_name, data_capture_configuration)\n",
    "except Exception as e:\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=endpoint)\n",
    "    endpoint_config = create_endpoint_config(training_job_name, data_capture_configuration)\n",
    "\n",
    "print('Endpoint configuration created: '+ endpoint_config['EndpointConfigArn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61c893cf-1f1e-43e1-9d6a-3471ace5eaf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint created: arn:aws:sagemaker:us-east-1:477728513638:endpoint/cc-training-job-1681895412\n"
     ]
    }
   ],
   "source": [
    "# Enable data capture, sampling 100% of the data for now. Next we deploy the endpoint in the correct VPC.\n",
    "\n",
    "endpoint_name = training_job_name\n",
    "def create_endpoint(endpoint_name, config_name):\n",
    "    return sm_client.create_endpoint(\n",
    "                                    EndpointName=endpoint_name,\n",
    "                                    EndpointConfigName=training_job_name\n",
    "                                )\n",
    "\n",
    "\n",
    "try:\n",
    "    endpoint = create_endpoint(endpoint_name, endpoint_config)\n",
    "except Exception as e:\n",
    "    sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint = create_endpoint(endpoint_name, endpoint_config)\n",
    "\n",
    "print('Endpoint created: '+ endpoint['EndpointArn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8793575-38d4-4570-8432-c27dfd6e8919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head -10 test_data.csv > test_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22b68f2c-8bff-460d-afdb-ce9e4ea274fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import RealTimePredictor\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, content_type = 'text/csv')\n",
    "\n",
    "with open('test_sample.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = predictor.predict(data=payload[2:])\n",
    "        sleep(0.5)\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91615e98-d500-4e22-800b-3cb7dbdf5aee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-modelmonitor/monitoring/datacapture/cc-training-job-1681895412/AllTraffic\n",
      "Found Capture Files:\n",
      "sagemaker-modelmonitor/monitoring/datacapture/cc-training-job-1681895412/AllTraffic/2023/04/19/10/45-47-798-cbb0ee13-5b78-4892-96d5-8cbdf11a9b69.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sagemaker-modelmonitor/monitoring/datacapture/cc-training-job-1681895412/AllTraffic/2023/04/19/10/45-47-798-cbb0ee13-5b78-4892-96d5-8cbdf11a9b69.jsonl'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the captured json files.\n",
    "data_capture_prefix = '{}/monitoring'.format(prefix)\n",
    "s3_client = boto3.Session().client('s3')\n",
    "current_endpoint_capture_prefix = '{}/datacapture/{}/AllTraffic'.format(data_capture_prefix, endpoint_name)\n",
    "print(current_endpoint_capture_prefix)\n",
    "result = s3_client.list_objects(Bucket=rawbucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))\n",
    "\n",
    "\n",
    "capture_files[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fabf0d49-adcf-4f7d-8c47-1c216fb6aa0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"data\": \"-0.34147611300851444,0.1932005252116958,50000.0,1.0,2.0,2.0,25.0,-1.0,3.0,2.0,0.0,0.0,0.0,10386.0,9993.0,9993.0,15300.0,0.0,0.0,200.0,5307.0,0.0,0.0\",\n",
      "      \"encoding\": \"CSV\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"observedContentType\": \"text/csv\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"data\": \"0.5108723044395447\",\n",
      "      \"encoding\": \"CSV\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"dab18f4a-3901-430e-9092-06d8a88b26c3\",\n",
      "    \"inferenceTime\": \"2023-04-19T10:45:50Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# View contents of the captured file.\n",
    "def get_obj_body(bucket, obj_key):\n",
    "    return s3_client.get_object(Bucket=rawbucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(rawbucket, capture_files[0])\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[5]), indent = 2, sort_keys =True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d757635-0572-423d-8fd2-4fb152107472",
   "metadata": {},
   "source": [
    "# Monitor endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5377c52f-0a91-4e36-9232-f6e78021697e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline data uri: s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/cc-training-job-1681895412/baselining/data\n",
      "Baseline results uri: s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/cc-training-job-1681895412/baselining/results\n",
      "s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/train_headers\n"
     ]
    }
   ],
   "source": [
    "model_prefix = prefix + \"/\" + endpoint_name\n",
    "baseline_prefix = model_prefix + '/baselining'\n",
    "baseline_data_prefix = baseline_prefix + '/data'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = 's3://{}/{}'.format(rawbucket,baseline_data_prefix)\n",
    "baseline_results_uri = 's3://{}/{}'.format(rawbucket, baseline_results_prefix)\n",
    "train_data_header_location = \"s3://\" + rawbucket + '/' + prefix + '/train_headers'\n",
    "print('Baseline data uri: {}'.format(baseline_data_uri))\n",
    "print('Baseline results uri: {}'.format(baseline_results_uri))\n",
    "print(train_data_header_location)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f51775b-9be3-41e7-84bc-af7db5391234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2023-04-19-10-48-16-968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  baseline-suggestion-job-2023-04-19-10-48-16-968\n",
      "Inputs:  [{'InputName': 'baseline_dataset_input', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/train_headers/train_data_with_headers.csv', 'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'monitoring_output', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/cc-training-job-1681895412/baselining/results', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".........................\u001b[34m2023-04-19 10:52:22,027 - matplotlib.font_manager - INFO - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:22.553662: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:22.553691: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:24.119002: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:24.119032: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:24.119058: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-82-53.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:24.119313: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:25,678 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:477728513638:processing-job/baseline-suggestion-job-2023-04-19-10-48-16-968', 'ProcessingJobName': 'baseline-suggestion-job-2023-04-19-10-48-16-968', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/train_headers/train_data_with_headers.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/cc-training-job-1681895412/baselining/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::477728513638:role/CFN-SM-IM-Lambda-Catalog-SageMakerExecutionRole-1GMGL94UP3F1R', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:25,678 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:25,678 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:25,678 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:25,679 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:25,735 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:25,736 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:25,736 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:25,744 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:25,744 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:25,744 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,210 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.82.53\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/had\u001b[0m\n",
      "\u001b[34moop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_362\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,220 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,224 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-089c4145-c688-443e-a43e-ad30b4fbfdc5\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,715 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,727 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,728 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,730 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,738 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,738 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,738 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,738 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,770 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,781 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,781 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,785 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,788 INFO blockmanagement.BlockManager: The block deletion will start around 2023 Apr 19 10:52:26\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,790 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,790 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,791 INFO util.GSet: 2.0% max memory 3.1 GB = 63.8 MB\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,791 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,866 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,869 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,869 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,870 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,870 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,870 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,870 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,870 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,870 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,870 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,870 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,870 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,895 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,895 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,895 INFO util.GSet: 1.0% max memory 3.1 GB = 31.9 MB\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,895 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,897 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,897 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,897 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,898 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,902 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,905 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,905 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,905 INFO util.GSet: 0.25% max memory 3.1 GB = 8.0 MB\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,905 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,912 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,912 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,912 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,915 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,916 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,917 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,917 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,917 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 979.8 KB\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,917 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,938 INFO namenode.FSImage: Allocated new BlockPoolId: BP-493350522-10.2.82.53-1681901546932\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,949 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:26,956 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:27,032 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:27,043 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:27,046 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.82.53\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:27,056 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:29,115 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:29,116 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:31,186 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:31,186 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:33,271 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:33,271 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:35,393 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:35,394 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:37,586 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:37,587 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:47,595 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:49,247 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:49,638 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:49,678 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:49,693 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,107 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,130 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,130 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,131 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,131 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,154 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11536, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,170 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,172 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,225 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,226 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,226 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,226 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,227 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,545 INFO util.Utils: Successfully started service 'sparkDriver' on port 45857.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,574 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,621 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,650 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,651 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,692 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,721 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-197f347f-fda0-4cf8-bf87-38dc76eb7aab\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,743 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,794 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:50,837 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.2.82.53:45857/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1681901570102\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:51,297 INFO client.RMProxy: Connecting to ResourceManager at /10.2.82.53:8032\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:52,149 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:52,149 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:52,155 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15731 MB per container)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:52,156 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:52,156 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:52,157 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:52,162 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:52,265 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:54,086 INFO yarn.Client: Uploading resource file:/tmp/spark-e52747e7-9f5b-48a3-8cd5-25f6f5ae947c/__spark_libs__746000417794673319.zip -> hdfs://10.2.82.53/user/root/.sparkStaging/application_1681901552794_0001/__spark_libs__746000417794673319.zip\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:55,754 INFO yarn.Client: Uploading resource file:/tmp/spark-e52747e7-9f5b-48a3-8cd5-25f6f5ae947c/__spark_conf__8298166962893752359.zip -> hdfs://10.2.82.53/user/root/.sparkStaging/application_1681901552794_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:55,804 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:55,804 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:55,804 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:55,804 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:55,804 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:55,832 INFO yarn.Client: Submitting application application_1681901552794_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:56,041 INFO impl.YarnClientImpl: Submitted application application_1681901552794_0001\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:57,057 INFO yarn.Client: Application report for application_1681901552794_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:57,060 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Wed Apr 19 10:52:57 +0000 2023] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1681901575936\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1681901552794_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:58,068 INFO yarn.Client: Application report for application_1681901552794_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:52:59,071 INFO yarn.Client: Application report for application_1681901552794_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:00,075 INFO yarn.Client: Application report for application_1681901552794_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:01,080 INFO yarn.Client: Application report for application_1681901552794_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:01,776 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1681901552794_0001), /proxy/application_1681901552794_0001\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:02,083 INFO yarn.Client: Application report for application_1681901552794_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:02,083 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.82.53\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1681901575936\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1681901552794_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:02,085 INFO cluster.YarnClientSchedulerBackend: Application application_1681901552794_0001 has started running.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:02,114 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37995.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:02,114 INFO netty.NettyBlockTransferService: Server created on 10.2.82.53:37995\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:02,123 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:02,154 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.82.53, 37995, None)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:02,162 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.82.53:37995 with 1458.6 MiB RAM, BlockManagerId(driver, 10.2.82.53, 37995, None)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:02,174 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.82.53, 37995, None)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:02,175 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.82.53, 37995, None)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:02,326 INFO util.log: Logging initialized @14490ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:03,254 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:07,601 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.82.53:34114) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:07,772 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:34963 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 34963, None)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:21,239 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:21,438 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:21,493 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:21,498 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:22,514 INFO datasources.InMemoryFileIndex: It took 48 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:22,692 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,067 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.3 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,070 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.82.53:37995 (size: 39.3 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,078 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,452 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,455 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,459 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 3887463\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,509 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,529 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,530 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,531 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,533 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,544 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,598 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,601 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,602 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.82.53:37995 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,603 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,619 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,620 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,665 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4636 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:23,924 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:34963 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:24,728 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:34963 (size: 39.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:25,034 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1383 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:25,036 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:25,049 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.472 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:25,058 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:25,058 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:25,061 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.551266 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:25,229 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.2.82.53:37995 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:25,230 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:34963 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,461 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,463 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,466 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Label: string, PAY_AMT1: string, BILL_AMT1: string, LIMIT_BAL: string, SEX: string ... 22 more fields>\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,722 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,756 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,757 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.82.53:37995 (size: 39.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,759 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:100\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,773 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,818 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:100\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,820 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:100) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,820 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:100)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,820 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,823 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,829 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:100), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,889 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 19.7 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,894 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.7 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,895 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.82.53:37995 (size: 8.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,896 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,897 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:100) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,897 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,901 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:27,959 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:34963 (size: 8.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:28,709 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:34963 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:29,536 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:34963 (size: 3.2 MiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:29,683 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1785 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:29,683 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:29,684 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:100) finished in 1.852 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:29,685 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:29,685 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:29,685 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:100, took 1.866638 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:29,978 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:34963 in memory (size: 8.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:29,982 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.2.82.53:37995 in memory (size: 8.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,029 INFO codegen.CodeGenerator: Code generated in 269.715449 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,533 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,657 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,661 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,662 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,662 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,664 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,666 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,686 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 116.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,688 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,689 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.82.53:37995 (size: 35.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,690 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,692 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,692 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,699 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:30,718 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:34963 (size: 35.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,180 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1483 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,180 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,182 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.512 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,182 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,183 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,183 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,185 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,262 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,264 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,264 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,264 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,264 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,265 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,277 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 169.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,280 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 46.6 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,280 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.82.53:37995 (size: 46.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,281 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,281 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,282 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,284 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,299 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:34963 (size: 46.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,342 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.2.82.53:34114\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,674 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 391 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,674 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,675 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.404 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,676 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,676 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,676 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.414181 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:32,738 INFO codegen.CodeGenerator: Code generated in 47.124175 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:33,162 INFO codegen.CodeGenerator: Code generated in 34.80079 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:33,237 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:33,238 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:33,239 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:33,239 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:33,240 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:33,241 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:33,253 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 40.3 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:33,255 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:33,256 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.2.82.53:37995 (size: 17.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:33,256 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:33,257 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:33,257 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:33,259 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:33,271 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:34963 (size: 17.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,240 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 981 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,241 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.998 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,241 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,241 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,241 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,242 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 1.004688 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,752 INFO codegen.CodeGenerator: Code generated in 101.430717 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,760 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,760 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,760 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,760 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,761 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,762 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,768 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 76.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,770 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,770 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.2.82.53:37995 (size: 24.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,771 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,771 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,771 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,773 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,789 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:34963 (size: 24.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,998 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 226 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:34,999 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,003 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.239 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,003 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,003 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,003 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,003 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,105 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:34963 in memory (size: 46.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,111 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.2.82.53:37995 in memory (size: 46.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,142 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:34963 in memory (size: 35.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,146 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.2.82.53:37995 in memory (size: 35.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,167 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.2.82.53:37995 in memory (size: 24.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,169 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:34963 in memory (size: 24.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,210 INFO codegen.CodeGenerator: Code generated in 89.466393 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,227 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:34963 in memory (size: 17.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,227 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,229 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,229 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,229 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,229 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,241 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,242 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.2.82.53:37995 in memory (size: 17.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,244 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,245 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,246 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.2.82.53:37995 (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,246 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,247 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,247 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,248 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,276 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:34963 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,281 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.82.53:34114\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,396 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 148 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,397 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,398 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.156 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,398 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,399 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,400 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.172047 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,459 INFO codegen.CodeGenerator: Code generated in 40.161221 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,650 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,666 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,667 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,667 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,667 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,667 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,673 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,686 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 32.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,687 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,688 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.2.82.53:37995 (size: 14.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,689 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,690 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,690 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,692 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:35,707 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:34963 (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,050 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1358 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,050 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,052 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.377 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,053 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,053 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,054 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,054 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,054 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,057 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,059 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,060 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.2.82.53:37995 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,061 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,061 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,062 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,064 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,077 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:34963 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,084 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.2.82.53:34114\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,125 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 62 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,126 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,127 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.072 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,127 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,128 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,128 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.477329 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,335 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,335 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,335 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,335 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,336 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,336 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,343 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 85.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,345 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,346 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.2.82.53:37995 (size: 27.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,347 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,347 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,347 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,349 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,361 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:34963 (size: 27.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,663 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 314 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,664 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,665 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.326 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,665 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,665 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,665 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,665 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,698 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,699 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,699 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,699 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,700 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,700 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,708 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 170.4 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,710 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 46.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,711 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.2.82.53:37995 (size: 46.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,711 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,712 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,712 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,713 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,724 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:34963 (size: 46.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,738 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.2.82.53:34114\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,878 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 165 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,878 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,879 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.176 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,880 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,881 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:37,881 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.183115 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,044 INFO codegen.CodeGenerator: Code generated in 15.061582 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,074 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,075 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,075 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,075 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,076 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,077 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,082 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 40.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,084 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,084 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.2.82.53:37995 (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,085 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,085 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,085 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,087 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,098 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:34963 (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,707 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 621 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,708 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.630 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,708 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,708 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,708 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,709 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.634126 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,965 INFO codegen.CodeGenerator: Code generated in 65.852484 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,972 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,973 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,973 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,973 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,974 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,975 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,980 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 75.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,982 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,983 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.2.82.53:37995 (size: 24.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,984 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,985 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,985 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,986 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:38,999 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:34963 (size: 24.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,191 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 205 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,192 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,193 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.216 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,194 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,195 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,195 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,195 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,380 INFO codegen.CodeGenerator: Code generated in 83.516999 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,393 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,395 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,395 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,395 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,396 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,396 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,398 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,402 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,403 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.2.82.53:37995 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,403 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,405 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,405 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,407 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,430 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:34963 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,438 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.2.82.53:34114\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,609 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 203 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,612 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,613 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.215 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,613 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,615 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,615 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.221384 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,774 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,775 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,777 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,777 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,777 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,778 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,781 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,798 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:34963 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,799 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.2.82.53:37995 in memory (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,808 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 32.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,809 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 14.7 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,810 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.2.82.53:37995 (size: 14.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,811 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,811 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,811 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,813 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,831 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:34963 (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,938 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.2.82.53:37995 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:39,947 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:34963 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,009 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 197 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,010 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,011 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.229 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,012 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,013 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,013 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,013 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,014 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,015 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,017 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,018 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.2.82.53:37995 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,018 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,019 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,021 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,023 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,025 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:34963 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,032 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.2.82.53:37995 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,057 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:34963 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,067 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.2.82.53:34114\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,081 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.2.82.53:37995 in memory (size: 46.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,098 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:34963 in memory (size: 46.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,107 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 84 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,107 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,108 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.094 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,108 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,108 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,109 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.334952 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,167 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:34963 in memory (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,197 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.2.82.53:37995 in memory (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,216 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:34963 in memory (size: 24.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,217 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.2.82.53:37995 in memory (size: 24.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,244 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.2.82.53:37995 in memory (size: 14.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,245 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:34963 in memory (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,292 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:34963 in memory (size: 27.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,299 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.2.82.53:37995 in memory (size: 27.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,371 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,371 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,371 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,371 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,372 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,372 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,376 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 85.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,378 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 27.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,379 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.2.82.53:37995 (size: 27.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,379 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,379 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,380 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,381 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,394 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:34963 (size: 27.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,625 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 244 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,625 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,626 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.253 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,626 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,626 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,626 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,627 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,656 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,657 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,657 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,657 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,657 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,657 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,662 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 170.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,664 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 46.8 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,664 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.2.82.53:37995 (size: 46.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,665 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,665 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,665 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,666 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,674 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:34963 (size: 46.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,683 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.2.82.53:34114\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,783 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 117 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,783 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,784 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.126 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,785 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,785 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,786 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.129845 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,882 INFO codegen.CodeGenerator: Code generated in 13.79771 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,905 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,906 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,906 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,906 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,907 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,907 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,913 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 40.3 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,915 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,915 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.2.82.53:37995 (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,916 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,916 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,916 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,917 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:40,932 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:34963 (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,469 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 552 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,469 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,470 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.562 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,470 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,471 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,471 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.565366 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,658 INFO codegen.CodeGenerator: Code generated in 44.071859 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,665 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,665 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,665 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,665 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,665 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,666 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,669 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 75.8 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,671 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,672 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.2.82.53:37995 (size: 24.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,672 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,675 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,675 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,676 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,686 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:34963 (size: 24.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,798 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 121 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,798 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,799 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.132 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,799 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,799 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,799 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,799 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,873 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,874 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,874 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,874 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,875 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,875 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,876 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 66.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,878 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,878 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.2.82.53:37995 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,879 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,879 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,880 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,884 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,892 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:34963 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,896 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.2.82.53:34114\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,902 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 18 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,902 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,903 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.028 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,904 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,904 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,905 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.031039 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,972 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,972 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,973 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,973 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,973 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,973 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,974 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,979 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 32.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,980 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 14.7 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,981 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.2.82.53:37995 (size: 14.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,982 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,982 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,982 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,984 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:41,994 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:34963 (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,084 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 101 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,084 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,086 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.110 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,086 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,086 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,087 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,087 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,087 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,089 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,091 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,091 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.2.82.53:37995 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,091 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,092 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,092 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,093 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,101 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:34963 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,103 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.2.82.53:34114\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,114 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 21 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,114 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,115 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.027 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,115 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,115 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,116 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.143885 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,197 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,197 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,197 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,197 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,198 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,198 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,202 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 85.7 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,203 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 28.0 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,204 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.2.82.53:37995 (size: 28.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,204 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,205 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,205 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,206 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,214 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:34963 (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,363 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 157 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,363 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,363 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.165 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,363 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,363 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,364 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,364 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,397 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,398 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,398 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,398 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,398 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,398 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,404 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 170.6 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,406 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 46.7 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,406 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.2.82.53:37995 (size: 46.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,406 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,407 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,407 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,408 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,419 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:34963 (size: 46.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,427 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.2.82.53:34114\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,491 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 83 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,491 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,492 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.093 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,492 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,492 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,493 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.095579 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,679 INFO codegen.CodeGenerator: Code generated in 18.208952 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,715 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,716 INFO scheduler.DAGScheduler: Got job 22 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,717 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,717 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,721 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,721 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,731 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 40.3 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,764 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,770 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.2.82.53:37995 (size: 16.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,771 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,771 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,772 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,773 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,782 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:34963 (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,829 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.2.82.53:37995 in memory (size: 16.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,842 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:34963 in memory (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,954 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.2.82.53:37995 in memory (size: 46.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:42,963 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:34963 in memory (size: 46.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,028 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.2.82.53:37995 in memory (size: 24.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,117 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:34963 in memory (size: 24.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,198 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.2.82.53:37995 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,201 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:34963 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,240 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:34963 in memory (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,249 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.2.82.53:37995 in memory (size: 28.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,290 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.2.82.53:37995 in memory (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,292 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:34963 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,331 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:34963 in memory (size: 46.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,336 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.2.82.53:37995 in memory (size: 46.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,367 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.2.82.53:37995 in memory (size: 27.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,378 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:34963 in memory (size: 27.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,390 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.2.82.53:37995 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,391 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:34963 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,414 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.2.82.53:37995 in memory (size: 14.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,428 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:34963 in memory (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,442 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.2.82.53:37995 in memory (size: 14.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,443 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:34963 in memory (size: 14.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,647 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 874 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,647 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,647 INFO scheduler.DAGScheduler: ResultStage 32 (treeReduce at KLLRunner.scala:107) finished in 0.924 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,648 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,648 INFO cluster.YarnScheduler: Killing all running tasks in stage 32: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,649 INFO scheduler.DAGScheduler: Job 22 finished: treeReduce at KLLRunner.scala:107, took 0.933351 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,809 INFO codegen.CodeGenerator: Code generated in 39.957172 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,815 INFO scheduler.DAGScheduler: Registering RDD 139 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,815 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,815 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,815 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,816 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,816 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,823 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 75.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,825 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,825 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.2.82.53:37995 (size: 23.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,826 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,826 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,826 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,828 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:43,840 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:34963 (size: 23.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,047 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 219 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,047 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,049 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326) finished in 0.232 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,049 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,049 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,049 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,049 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,162 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,163 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,163 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,164 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,164 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,164 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,167 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 66.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,170 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,171 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.2.82.53:37995 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,173 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,173 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,173 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,175 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,185 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:34963 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,189 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.2.82.53:34114\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,202 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 27 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,202 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,203 INFO scheduler.DAGScheduler: ResultStage 35 (collect at AnalysisRunner.scala:326) finished in 0.036 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,203 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,203 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,204 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.041329 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,431 INFO scheduler.DAGScheduler: Registering RDD 147 (collect at AnalysisRunner.scala:326) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,431 INFO scheduler.DAGScheduler: Got map stage job 25 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,431 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 36 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,431 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,432 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,432 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[147] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,435 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 75.6 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,437 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 25.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,438 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.2.82.53:37995 (size: 25.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,438 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,439 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[147] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,439 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,440 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,459 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:34963 (size: 25.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,744 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 304 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,745 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,746 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (collect at AnalysisRunner.scala:326) finished in 0.313 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,746 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,746 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,746 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,746 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,781 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,782 INFO scheduler.DAGScheduler: Got job 26 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,783 INFO scheduler.DAGScheduler: Final stage: ResultStage 38 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,783 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,783 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,784 INFO scheduler.DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[150] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,791 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 144.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,793 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 40.8 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,794 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.2.82.53:37995 (size: 40.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,795 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,795 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[150] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,796 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,797 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 29) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,810 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:34963 (size: 40.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,824 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.2.82.53:34114\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,897 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 29) in 100 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,897 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,898 INFO scheduler.DAGScheduler: ResultStage 38 (collect at AnalysisRunner.scala:326) finished in 0.112 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,899 INFO scheduler.DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,900 INFO cluster.YarnScheduler: Killing all running tasks in stage 38: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,900 INFO scheduler.DAGScheduler: Job 26 finished: collect at AnalysisRunner.scala:326, took 0.118553 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,925 INFO codegen.CodeGenerator: Code generated in 23.096485 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:44,990 INFO codegen.CodeGenerator: Code generated in 12.915607 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,039 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,040 INFO scheduler.DAGScheduler: Got job 27 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,040 INFO scheduler.DAGScheduler: Final stage: ResultStage 39 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,040 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,041 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,041 INFO scheduler.DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[160] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,046 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 39.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,048 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,048 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.2.82.53:37995 (size: 16.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,049 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,050 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[160] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,050 INFO cluster.YarnScheduler: Adding task set 39.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,051 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 39.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,062 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:34963 (size: 16.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,571 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 39.0 (TID 30) in 520 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,572 INFO scheduler.DAGScheduler: ResultStage 39 (treeReduce at KLLRunner.scala:107) finished in 0.529 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,572 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,572 INFO cluster.YarnScheduler: Removed TaskSet 39.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,572 INFO cluster.YarnScheduler: Killing all running tasks in stage 39: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,572 INFO scheduler.DAGScheduler: Job 27 finished: treeReduce at KLLRunner.scala:107, took 0.532518 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,763 INFO codegen.CodeGenerator: Code generated in 71.292941 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,769 INFO scheduler.DAGScheduler: Registering RDD 165 (collect at AnalysisRunner.scala:326) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,769 INFO scheduler.DAGScheduler: Got map stage job 28 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,769 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 40 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,769 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,770 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,770 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[165] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,775 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 65.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,777 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 21.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,777 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.2.82.53:37995 (size: 21.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,778 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,778 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[165] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,779 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,780 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,795 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:34963 (size: 21.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,938 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 158 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,938 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,939 INFO scheduler.DAGScheduler: ShuffleMapStage 40 (collect at AnalysisRunner.scala:326) finished in 0.167 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,941 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,941 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,942 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:45,942 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,044 INFO codegen.CodeGenerator: Code generated in 58.39288 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,061 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,062 INFO scheduler.DAGScheduler: Got job 29 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,063 INFO scheduler.DAGScheduler: Final stage: ResultStage 42 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,063 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,063 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,064 INFO scheduler.DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[168] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,068 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 55.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,070 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,073 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.2.82.53:37995 (size: 16.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,073 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,074 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[168] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,074 INFO cluster.YarnScheduler: Adding task set 42.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,075 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 32) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,087 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-1:34963 (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,091 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.2.82.53:34114\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,146 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 32) in 71 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,147 INFO cluster.YarnScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,148 INFO scheduler.DAGScheduler: ResultStage 42 (collect at AnalysisRunner.scala:326) finished in 0.081 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,149 INFO scheduler.DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,149 INFO cluster.YarnScheduler: Killing all running tasks in stage 42: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,150 INFO scheduler.DAGScheduler: Job 29 finished: collect at AnalysisRunner.scala:326, took 0.088140 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,191 INFO codegen.CodeGenerator: Code generated in 32.010509 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,424 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,450 INFO codegen.CodeGenerator: Code generated in 7.018128 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,455 INFO scheduler.DAGScheduler: Registering RDD 173 (count at StatsGenerator.scala:66) as input to shuffle 13\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,455 INFO scheduler.DAGScheduler: Got map stage job 30 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,455 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 43 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,455 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,456 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,456 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 43 (MapPartitionsRDD[173] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,458 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 24.7 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,461 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,461 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.2.82.53:37995 (size: 10.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,462 INFO spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,462 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[173] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,463 INFO cluster.YarnScheduler: Adding task set 43.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,464 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 43.0 (TID 33) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,472 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on algo-1:34963 (size: 10.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,505 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 43.0 (TID 33) in 41 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,505 INFO cluster.YarnScheduler: Removed TaskSet 43.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,506 INFO scheduler.DAGScheduler: ShuffleMapStage 43 (count at StatsGenerator.scala:66) finished in 0.050 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,506 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,506 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,506 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,508 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,525 INFO codegen.CodeGenerator: Code generated in 10.650767 ms\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,536 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,537 INFO scheduler.DAGScheduler: Got job 31 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,538 INFO scheduler.DAGScheduler: Final stage: ResultStage 45 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,538 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,538 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,538 INFO scheduler.DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[176] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,541 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 11.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,545 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,545 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.2.82.53:37995 (size: 5.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,546 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,546 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[176] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,546 INFO cluster.YarnScheduler: Adding task set 45.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,547 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 34) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,557 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on algo-1:34963 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,561 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.2.82.53:34114\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,575 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 34) in 28 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,575 INFO cluster.YarnScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,576 INFO scheduler.DAGScheduler: ResultStage 45 (count at StatsGenerator.scala:66) finished in 0.037 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,577 INFO scheduler.DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,578 INFO cluster.YarnScheduler: Killing all running tasks in stage 45: Stage finished\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,578 INFO scheduler.DAGScheduler: Job 31 finished: count at StatsGenerator.scala:66, took 0.041374 s\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,670 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:34963 in memory (size: 23.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,671 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.2.82.53:37995 in memory (size: 23.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,676 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:34963 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,682 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.2.82.53:37995 in memory (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,691 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on algo-1:34963 in memory (size: 21.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,697 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.2.82.53:37995 in memory (size: 21.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,701 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.2.82.53:37995 in memory (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,702 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on algo-1:34963 in memory (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,705 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.2.82.53:37995 in memory (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,706 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:34963 in memory (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,709 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.2.82.53:37995 in memory (size: 16.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,711 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on algo-1:34963 in memory (size: 16.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,721 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.2.82.53:37995 in memory (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,722 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on algo-1:34963 in memory (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,724 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.2.82.53:37995 in memory (size: 40.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,725 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on algo-1:34963 in memory (size: 40.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,727 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.2.82.53:37995 in memory (size: 25.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,728 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:34963 in memory (size: 25.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,732 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on 10.2.82.53:37995 in memory (size: 10.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:46,732 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on algo-1:34963 in memory (size: 10.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,231 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,260 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,316 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,317 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,323 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,343 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,380 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,381 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,400 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,415 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,472 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,473 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,473 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,522 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,523 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-e52747e7-9f5b-48a3-8cd5-25f6f5ae947c\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,530 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-1a851012-b22a-4258-8d5e-a4096d3167d3\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,597 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2023-04-19 10:53:47,597 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7f181265d510>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600)\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=os.path.join(train_data_header_location, 'train_data_with_headers.csv'),\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eef39d58-905f-4011-a3b0-5c889b0a645b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Files:\n",
      "sagemaker-modelmonitor/cc-training-job-1681895412/baselining/results/constraints.json\n",
      " sagemaker-modelmonitor/cc-training-job-1681895412/baselining/results/statistics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>numerical_statistics.common.num_present</th>\n",
       "      <th>numerical_statistics.common.num_missing</th>\n",
       "      <th>numerical_statistics.mean</th>\n",
       "      <th>numerical_statistics.sum</th>\n",
       "      <th>numerical_statistics.std_dev</th>\n",
       "      <th>numerical_statistics.min</th>\n",
       "      <th>numerical_statistics.max</th>\n",
       "      <th>numerical_statistics.distribution.kll.buckets</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.c</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.k</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Label</td>\n",
       "      <td>Integral</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.224583e-01</td>\n",
       "      <td>5.339000e+03</td>\n",
       "      <td>0.415897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PAY_AMT1</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.965234e-16</td>\n",
       "      <td>4.716560e-12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.341476</td>\n",
       "      <td>5.223016e+01</td>\n",
       "      <td>[{'lower_bound': -0.34147611300851444, 'upper_...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-0.16093173468294658, -0.2812946535666585, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BILL_AMT1</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.378091e-01</td>\n",
       "      <td>5.707419e+03</td>\n",
       "      <td>0.080585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.0999999...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.19552189076210494, 0.2369590330493186, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIMIT_BAL</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.677310e+05</td>\n",
       "      <td>4.025544e+09</td>\n",
       "      <td>129479.698677</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>8.000000e+05</td>\n",
       "      <td>[{'lower_bound': 10000.0, 'upper_bound': 89000...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[30000.0, 120000.0, 200000.0, 130000.0, 28000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SEX</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.601167e+00</td>\n",
       "      <td>3.842800e+04</td>\n",
       "      <td>0.489658</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>[{'lower_bound': 1.0, 'upper_bound': 1.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.851083e+00</td>\n",
       "      <td>4.442600e+04</td>\n",
       "      <td>0.788030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.6, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MARRIAGE</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.553125e+00</td>\n",
       "      <td>3.727500e+04</td>\n",
       "      <td>0.521227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.3, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AGE</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>3.545954e+01</td>\n",
       "      <td>8.510290e+05</td>\n",
       "      <td>9.191179</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.900000e+01</td>\n",
       "      <td>[{'lower_bound': 21.0, 'upper_bound': 26.8, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[26.0, 27.0, 58.0, 29.0, 49.0, 23.0, 26.0, 29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PAY_0</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.316667e-02</td>\n",
       "      <td>-3.160000e+02</td>\n",
       "      <td>1.127531</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-1.0, 0.0, 1.0, 0.0, -2.0, -1.0, -1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PAY_2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.276667e-01</td>\n",
       "      <td>-3.064000e+03</td>\n",
       "      <td>1.199702</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.1, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 2.0, 0.0, -2.0, -1.0, 0.0, 0.0, 5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PAY_3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.595833e-01</td>\n",
       "      <td>-3.830000e+03</td>\n",
       "      <td>1.198833</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, -1.0, 2.0, 0.0, -2.0, -1.0, 0.0, 0.0, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PAY_4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.157500e-01</td>\n",
       "      <td>-5.178000e+03</td>\n",
       "      <td>1.168026</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, -1.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PAY_5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.625833e-01</td>\n",
       "      <td>-6.302000e+03</td>\n",
       "      <td>1.130582</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-1.0, -2.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PAY_6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.858750e-01</td>\n",
       "      <td>-6.861000e+03</td>\n",
       "      <td>1.148760</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>[{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[-1.0, -2.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BILL_AMT2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.929893e+04</td>\n",
       "      <td>1.183174e+09</td>\n",
       "      <td>70960.574221</td>\n",
       "      <td>-69777.000000</td>\n",
       "      <td>7.439700e+05</td>\n",
       "      <td>[{'lower_bound': -69777.0, 'upper_bound': 1159...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[14899.0, 15000.0, 206084.0, 67664.0, 6163.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BILL_AMT3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.717089e+04</td>\n",
       "      <td>1.132101e+09</td>\n",
       "      <td>69505.298901</td>\n",
       "      <td>-157264.000000</td>\n",
       "      <td>1.664089e+06</td>\n",
       "      <td>[{'lower_bound': -157264.0, 'upper_bound': 248...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[16790.0, 100.0, 187747.0, 66258.0, -54.0, 44...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BILL_AMT4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.325871e+04</td>\n",
       "      <td>1.038209e+09</td>\n",
       "      <td>64153.274849</td>\n",
       "      <td>-170000.000000</td>\n",
       "      <td>7.068640e+05</td>\n",
       "      <td>[{'lower_bound': -170000.0, 'upper_bound': -82...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[12400.0, 0.0, 159746.0, 62697.0, -54.0, 4536...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BILL_AMT5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.036543e+04</td>\n",
       "      <td>9.687702e+08</td>\n",
       "      <td>60673.598646</td>\n",
       "      <td>-81334.000000</td>\n",
       "      <td>8.235400e+05</td>\n",
       "      <td>[{'lower_bound': -81334.0, 'upper_bound': 9153...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[780.0, 0.0, 140632.0, 48210.0, -54.0, 46135....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BILL_AMT6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>3.891905e+04</td>\n",
       "      <td>9.340573e+08</td>\n",
       "      <td>59424.681250</td>\n",
       "      <td>-339603.000000</td>\n",
       "      <td>6.999440e+05</td>\n",
       "      <td>[{'lower_bound': -339603.0, 'upper_bound': -23...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 143092.0, 46255.0, -54.0, 46029.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PAY_AMT2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>5.926893e+03</td>\n",
       "      <td>1.422454e+08</td>\n",
       "      <td>23436.624063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.684259e+06</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 168425.9,...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[3000.0, 100.0, 0.0, 3000.0, 0.0, 45361.0, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PAY_AMT3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>5.186248e+03</td>\n",
       "      <td>1.244700e+08</td>\n",
       "      <td>17186.818924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.890430e+05</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 88904.3, ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[6000.0, 0.0, 6014.0, 6000.0, 0.0, 1800.0, 65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PAY_AMT4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.830179e+03</td>\n",
       "      <td>1.159243e+08</td>\n",
       "      <td>15482.205461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.210000e+05</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 62100.0, ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[780.0, 0.0, 5000.0, 2000.0, 0.0, 1503.0, 706...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PAY_AMT5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.797411e+03</td>\n",
       "      <td>1.151379e+08</td>\n",
       "      <td>15166.388154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.880710e+05</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 38807.1, ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 6000.0, 2000.0, 0.0, 1735.0, 954.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PAY_AMT6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>24000</td>\n",
       "      <td>0</td>\n",
       "      <td>5.219489e+03</td>\n",
       "      <td>1.252677e+08</td>\n",
       "      <td>17606.700346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.271430e+05</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 52714.3, ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 5000.0, 2000.0, 0.0, 1724.0, 700.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name inferred_type  numerical_statistics.common.num_present  \\\n",
       "0       Label      Integral                                    24000   \n",
       "1    PAY_AMT1    Fractional                                    24000   \n",
       "2   BILL_AMT1    Fractional                                    24000   \n",
       "3   LIMIT_BAL    Fractional                                    24000   \n",
       "4         SEX    Fractional                                    24000   \n",
       "5   EDUCATION    Fractional                                    24000   \n",
       "6    MARRIAGE    Fractional                                    24000   \n",
       "7         AGE    Fractional                                    24000   \n",
       "8       PAY_0    Fractional                                    24000   \n",
       "9       PAY_2    Fractional                                    24000   \n",
       "10      PAY_3    Fractional                                    24000   \n",
       "11      PAY_4    Fractional                                    24000   \n",
       "12      PAY_5    Fractional                                    24000   \n",
       "13      PAY_6    Fractional                                    24000   \n",
       "14  BILL_AMT2    Fractional                                    24000   \n",
       "15  BILL_AMT3    Fractional                                    24000   \n",
       "16  BILL_AMT4    Fractional                                    24000   \n",
       "17  BILL_AMT5    Fractional                                    24000   \n",
       "18  BILL_AMT6    Fractional                                    24000   \n",
       "19   PAY_AMT2    Fractional                                    24000   \n",
       "20   PAY_AMT3    Fractional                                    24000   \n",
       "21   PAY_AMT4    Fractional                                    24000   \n",
       "22   PAY_AMT5    Fractional                                    24000   \n",
       "23   PAY_AMT6    Fractional                                    24000   \n",
       "\n",
       "    numerical_statistics.common.num_missing  numerical_statistics.mean  \\\n",
       "0                                         0               2.224583e-01   \n",
       "1                                         0               1.965234e-16   \n",
       "2                                         0               2.378091e-01   \n",
       "3                                         0               1.677310e+05   \n",
       "4                                         0               1.601167e+00   \n",
       "5                                         0               1.851083e+00   \n",
       "6                                         0               1.553125e+00   \n",
       "7                                         0               3.545954e+01   \n",
       "8                                         0              -1.316667e-02   \n",
       "9                                         0              -1.276667e-01   \n",
       "10                                        0              -1.595833e-01   \n",
       "11                                        0              -2.157500e-01   \n",
       "12                                        0              -2.625833e-01   \n",
       "13                                        0              -2.858750e-01   \n",
       "14                                        0               4.929893e+04   \n",
       "15                                        0               4.717089e+04   \n",
       "16                                        0               4.325871e+04   \n",
       "17                                        0               4.036543e+04   \n",
       "18                                        0               3.891905e+04   \n",
       "19                                        0               5.926893e+03   \n",
       "20                                        0               5.186248e+03   \n",
       "21                                        0               4.830179e+03   \n",
       "22                                        0               4.797411e+03   \n",
       "23                                        0               5.219489e+03   \n",
       "\n",
       "    numerical_statistics.sum  numerical_statistics.std_dev  \\\n",
       "0               5.339000e+03                      0.415897   \n",
       "1               4.716560e-12                      1.000000   \n",
       "2               5.707419e+03                      0.080585   \n",
       "3               4.025544e+09                 129479.698677   \n",
       "4               3.842800e+04                      0.489658   \n",
       "5               4.442600e+04                      0.788030   \n",
       "6               3.727500e+04                      0.521227   \n",
       "7               8.510290e+05                      9.191179   \n",
       "8              -3.160000e+02                      1.127531   \n",
       "9              -3.064000e+03                      1.199702   \n",
       "10             -3.830000e+03                      1.198833   \n",
       "11             -5.178000e+03                      1.168026   \n",
       "12             -6.302000e+03                      1.130582   \n",
       "13             -6.861000e+03                      1.148760   \n",
       "14              1.183174e+09                  70960.574221   \n",
       "15              1.132101e+09                  69505.298901   \n",
       "16              1.038209e+09                  64153.274849   \n",
       "17              9.687702e+08                  60673.598646   \n",
       "18              9.340573e+08                  59424.681250   \n",
       "19              1.422454e+08                  23436.624063   \n",
       "20              1.244700e+08                  17186.818924   \n",
       "21              1.159243e+08                  15482.205461   \n",
       "22              1.151379e+08                  15166.388154   \n",
       "23              1.252677e+08                  17606.700346   \n",
       "\n",
       "    numerical_statistics.min  numerical_statistics.max  \\\n",
       "0                   0.000000              1.000000e+00   \n",
       "1                  -0.341476              5.223016e+01   \n",
       "2                   0.000000              1.000000e+00   \n",
       "3               10000.000000              8.000000e+05   \n",
       "4                   1.000000              2.000000e+00   \n",
       "5                   0.000000              6.000000e+00   \n",
       "6                   0.000000              3.000000e+00   \n",
       "7                  21.000000              7.900000e+01   \n",
       "8                  -2.000000              8.000000e+00   \n",
       "9                  -2.000000              7.000000e+00   \n",
       "10                 -2.000000              8.000000e+00   \n",
       "11                 -2.000000              8.000000e+00   \n",
       "12                 -2.000000              8.000000e+00   \n",
       "13                 -2.000000              8.000000e+00   \n",
       "14             -69777.000000              7.439700e+05   \n",
       "15            -157264.000000              1.664089e+06   \n",
       "16            -170000.000000              7.068640e+05   \n",
       "17             -81334.000000              8.235400e+05   \n",
       "18            -339603.000000              6.999440e+05   \n",
       "19                  0.000000              1.684259e+06   \n",
       "20                  0.000000              8.890430e+05   \n",
       "21                  0.000000              6.210000e+05   \n",
       "22                  0.000000              3.880710e+05   \n",
       "23                  0.000000              5.271430e+05   \n",
       "\n",
       "        numerical_statistics.distribution.kll.buckets  \\\n",
       "0   [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "1   [{'lower_bound': -0.34147611300851444, 'upper_...   \n",
       "2   [{'lower_bound': 0.0, 'upper_bound': 0.0999999...   \n",
       "3   [{'lower_bound': 10000.0, 'upper_bound': 89000...   \n",
       "4   [{'lower_bound': 1.0, 'upper_bound': 1.1, 'cou...   \n",
       "5   [{'lower_bound': 0.0, 'upper_bound': 0.6, 'cou...   \n",
       "6   [{'lower_bound': 0.0, 'upper_bound': 0.3, 'cou...   \n",
       "7   [{'lower_bound': 21.0, 'upper_bound': 26.8, 'c...   \n",
       "8   [{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...   \n",
       "9   [{'lower_bound': -2.0, 'upper_bound': -1.1, 'c...   \n",
       "10  [{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...   \n",
       "11  [{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...   \n",
       "12  [{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...   \n",
       "13  [{'lower_bound': -2.0, 'upper_bound': -1.0, 'c...   \n",
       "14  [{'lower_bound': -69777.0, 'upper_bound': 1159...   \n",
       "15  [{'lower_bound': -157264.0, 'upper_bound': 248...   \n",
       "16  [{'lower_bound': -170000.0, 'upper_bound': -82...   \n",
       "17  [{'lower_bound': -81334.0, 'upper_bound': 9153...   \n",
       "18  [{'lower_bound': -339603.0, 'upper_bound': -23...   \n",
       "19  [{'lower_bound': 0.0, 'upper_bound': 168425.9,...   \n",
       "20  [{'lower_bound': 0.0, 'upper_bound': 88904.3, ...   \n",
       "21  [{'lower_bound': 0.0, 'upper_bound': 62100.0, ...   \n",
       "22  [{'lower_bound': 0.0, 'upper_bound': 38807.1, ...   \n",
       "23  [{'lower_bound': 0.0, 'upper_bound': 52714.3, ...   \n",
       "\n",
       "    numerical_statistics.distribution.kll.sketch.parameters.c  \\\n",
       "0                                                0.64           \n",
       "1                                                0.64           \n",
       "2                                                0.64           \n",
       "3                                                0.64           \n",
       "4                                                0.64           \n",
       "5                                                0.64           \n",
       "6                                                0.64           \n",
       "7                                                0.64           \n",
       "8                                                0.64           \n",
       "9                                                0.64           \n",
       "10                                               0.64           \n",
       "11                                               0.64           \n",
       "12                                               0.64           \n",
       "13                                               0.64           \n",
       "14                                               0.64           \n",
       "15                                               0.64           \n",
       "16                                               0.64           \n",
       "17                                               0.64           \n",
       "18                                               0.64           \n",
       "19                                               0.64           \n",
       "20                                               0.64           \n",
       "21                                               0.64           \n",
       "22                                               0.64           \n",
       "23                                               0.64           \n",
       "\n",
       "    numerical_statistics.distribution.kll.sketch.parameters.k  \\\n",
       "0                                              2048.0           \n",
       "1                                              2048.0           \n",
       "2                                              2048.0           \n",
       "3                                              2048.0           \n",
       "4                                              2048.0           \n",
       "5                                              2048.0           \n",
       "6                                              2048.0           \n",
       "7                                              2048.0           \n",
       "8                                              2048.0           \n",
       "9                                              2048.0           \n",
       "10                                             2048.0           \n",
       "11                                             2048.0           \n",
       "12                                             2048.0           \n",
       "13                                             2048.0           \n",
       "14                                             2048.0           \n",
       "15                                             2048.0           \n",
       "16                                             2048.0           \n",
       "17                                             2048.0           \n",
       "18                                             2048.0           \n",
       "19                                             2048.0           \n",
       "20                                             2048.0           \n",
       "21                                             2048.0           \n",
       "22                                             2048.0           \n",
       "23                                             2048.0           \n",
       "\n",
       "    numerical_statistics.distribution.kll.sketch.data  \n",
       "0   [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1   [[-0.16093173468294658, -0.2812946535666585, 0...  \n",
       "2   [[0.19552189076210494, 0.2369590330493186, 0.4...  \n",
       "3   [[30000.0, 120000.0, 200000.0, 130000.0, 28000...  \n",
       "4   [[1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0,...  \n",
       "5   [[2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0,...  \n",
       "6   [[2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0,...  \n",
       "7   [[26.0, 27.0, 58.0, 29.0, 49.0, 23.0, 26.0, 29...  \n",
       "8   [[-1.0, 0.0, 1.0, 0.0, -2.0, -1.0, -1.0, 0.0, ...  \n",
       "9   [[0.0, 0.0, 2.0, 0.0, -2.0, -1.0, 0.0, 0.0, 5....  \n",
       "10  [[0.0, -1.0, 2.0, 0.0, -2.0, -1.0, 0.0, 0.0, 4...  \n",
       "11  [[0.0, -1.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 3....  \n",
       "12  [[-1.0, -2.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 2...  \n",
       "13  [[-1.0, -2.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 0...  \n",
       "14  [[14899.0, 15000.0, 206084.0, 67664.0, 6163.0,...  \n",
       "15  [[16790.0, 100.0, 187747.0, 66258.0, -54.0, 44...  \n",
       "16  [[12400.0, 0.0, 159746.0, 62697.0, -54.0, 4536...  \n",
       "17  [[780.0, 0.0, 140632.0, 48210.0, -54.0, 46135....  \n",
       "18  [[0.0, 0.0, 143092.0, 46255.0, -54.0, 46029.0,...  \n",
       "19  [[3000.0, 100.0, 0.0, 3000.0, 0.0, 45361.0, 12...  \n",
       "20  [[6000.0, 0.0, 6014.0, 6000.0, 0.0, 1800.0, 65...  \n",
       "21  [[780.0, 0.0, 5000.0, 2000.0, 0.0, 1503.0, 706...  \n",
       "22  [[0.0, 0.0, 6000.0, 2000.0, 0.0, 1735.0, 954.0...  \n",
       "23  [[0.0, 0.0, 5000.0, 2000.0, 0.0, 1724.0, 700.0...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=rawbucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81b4c567-f46e-49d5-96d2-193ab14f1142",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>completeness</th>\n",
       "      <th>num_constraints.is_non_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Label</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PAY_AMT1</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BILL_AMT1</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIMIT_BAL</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SEX</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MARRIAGE</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AGE</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PAY_0</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PAY_2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PAY_3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PAY_4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PAY_5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PAY_6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BILL_AMT2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BILL_AMT3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BILL_AMT4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BILL_AMT5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BILL_AMT6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PAY_AMT2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PAY_AMT3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PAY_AMT4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PAY_AMT5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PAY_AMT6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name inferred_type  completeness  num_constraints.is_non_negative\n",
       "0       Label      Integral           1.0                             True\n",
       "1    PAY_AMT1    Fractional           1.0                            False\n",
       "2   BILL_AMT1    Fractional           1.0                             True\n",
       "3   LIMIT_BAL    Fractional           1.0                             True\n",
       "4         SEX    Fractional           1.0                             True\n",
       "5   EDUCATION    Fractional           1.0                             True\n",
       "6    MARRIAGE    Fractional           1.0                             True\n",
       "7         AGE    Fractional           1.0                             True\n",
       "8       PAY_0    Fractional           1.0                            False\n",
       "9       PAY_2    Fractional           1.0                            False\n",
       "10      PAY_3    Fractional           1.0                            False\n",
       "11      PAY_4    Fractional           1.0                            False\n",
       "12      PAY_5    Fractional           1.0                            False\n",
       "13      PAY_6    Fractional           1.0                            False\n",
       "14  BILL_AMT2    Fractional           1.0                            False\n",
       "15  BILL_AMT3    Fractional           1.0                            False\n",
       "16  BILL_AMT4    Fractional           1.0                            False\n",
       "17  BILL_AMT5    Fractional           1.0                            False\n",
       "18  BILL_AMT6    Fractional           1.0                            False\n",
       "19   PAY_AMT2    Fractional           1.0                             True\n",
       "20   PAY_AMT3    Fractional           1.0                             True\n",
       "21   PAY_AMT4    Fractional           1.0                             True\n",
       "22   PAY_AMT5    Fractional           1.0                             True\n",
       "23   PAY_AMT6    Fractional           1.0                             True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b829747f-703c-4fbc-a215-21182e604bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating monitoring schedule name Built-train-deploy-model-monitor-schedule-2023-04-19-10-55-13.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Monitoring Schedule with name: Built-train-deploy-model-monitor-schedule-2023-04-19-10-55-13\n"
     ]
    }
   ],
   "source": [
    "reports_prefix = '{}/reports'.format(prefix)\n",
    "s3_report_path = 's3://{}/{}'.format(rawbucket,reports_prefix)\n",
    "print(s3_report_path)\n",
    "\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "mon_schedule_name = 'Built-train-deploy-model-monitor-schedule-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=predictor.endpoint,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b28e52-f8e5-4eee-950c-15165afe6db0",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b4b5dc9c-823c-41bc-a563-806df86212cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting Monitoring Schedule with name: None\n"
     ]
    },
    {
     "ename": "ParamValidationError",
     "evalue": "Parameter validation failed:\nInvalid type for parameter MonitoringScheduleName, value: None, type: <class 'NoneType'>, valid types: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParamValidationError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-be878c6feb36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_default_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_monitoring_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# actually wait for the deletion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/model_monitor/model_monitoring.py\u001b[0m in \u001b[0;36mdelete_monitoring_schedule\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;34m\"\"\"Deletes the monitoring schedule.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         self.sagemaker_session.delete_monitoring_schedule(\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0mmonitoring_schedule_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitoring_schedule_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         )\n\u001b[1;32m    503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitoring_schedule_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mdelete_monitoring_schedule\u001b[0;34m(self, monitoring_schedule_name)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Deleting Monitoring Schedule with name: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitoring_schedule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         self.sagemaker_client.delete_monitoring_schedule(\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0mMonitoringScheduleName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmonitoring_schedule_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m         )\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m                 )\n\u001b[1;32m    529\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    922\u001b[0m             \u001b[0mendpoint_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madditional_headers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m         )\n\u001b[1;32m    926\u001b[0m         \u001b[0mresolve_checksum_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_convert_to_request_dict\u001b[0;34m(self, api_params, operation_model, endpoint_url, context, headers, set_user_agent_header)\u001b[0m\n\u001b[1;32m    989\u001b[0m         )\n\u001b[1;32m    990\u001b[0m         request_dict = self._serializer.serialize_to_request(\n\u001b[0;32m--> 991\u001b[0;31m             \u001b[0mapi_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m         )\n\u001b[1;32m    993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minject_host_prefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/validate.py\u001b[0m in \u001b[0;36mserialize_to_request\u001b[0;34m(self, parameters, operation_model)\u001b[0m\n\u001b[1;32m    379\u001b[0m             )\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParamValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         return self._serializer.serialize_to_request(\n\u001b[1;32m    383\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParamValidationError\u001b[0m: Parameter validation failed:\nInvalid type for parameter MonitoringScheduleName, value: None, type: <class 'NoneType'>, valid types: <class 'str'>"
     ]
    }
   ],
   "source": [
    "my_default_monitor.delete_monitoring_schedule()\n",
    "time.sleep(10) # actually wait for the deletion\n",
    "sm.delete_endpoint(EndpointName = endpoint_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fdb87411-f941-462b-a5c8-620a962db5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/data/rawdata.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "aws s3 rm --recursive s3://sagemaker-us-east-1-477728513638/sagemaker-modelmonitor/data"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
